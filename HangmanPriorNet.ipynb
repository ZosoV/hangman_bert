{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict, Dataset\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Set a seed for all libraries\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word  word_length  unique_chars\n",
      "0      timpani            7             6\n",
      "1       worsle            6             6\n",
      "2        yinst            5             5\n",
      "3  grangerized           11             8\n",
      "4      matatua            7             4\n",
      "(204570, 3)\n",
      "(22730, 3)\n",
      "(130436, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets \n",
    "df_train = pd.read_csv('data/to_mask_train.csv', converters={'word': str})\n",
    "df_val = pd.read_csv('data/masked_val.csv', converters={'masked_word': str, 'labels': str, 'previous_guesses': eval})\n",
    "df_test = pd.read_csv('data/masked_test.csv', converters={'masked_word': str, 'labels': str, 'previous_guesses': eval})\n",
    "\n",
    "print(df_train.head())\n",
    "\n",
    "# Add spaces to the word in training\n",
    "df_train[\"word\"] = df_train[\"word\"].apply(lambda x: ' '.join(list(x)))\n",
    "\n",
    "# Add spaces to the masked_word and labels\n",
    "df_val[\"masked_word\"] = df_val[\"masked_word\"].apply(lambda x: ' '.join(list(x)))\n",
    "df_val[\"labels\"] = df_val[\"labels\"].apply(lambda x: ' '.join(list(x)))\n",
    "df_test[\"masked_word\"] = df_test[\"masked_word\"].apply(lambda x: ' '.join(list(x)))\n",
    "df_test[\"labels\"] = df_test[\"labels\"].apply(lambda x: ' '.join(list(x)))\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>word_length</th>\n",
       "      <th>unique_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t i m p a n i</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>w o r s l e</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>y i n s t</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g r a n g e r i z e d</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m a t a t u a</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    word  word_length  unique_chars\n",
       "0          t i m p a n i            7             6\n",
       "1            w o r s l e            6             6\n",
       "2              y i n s t            5             5\n",
       "3  g r a n g e r i z e d           11             8\n",
       "4          m a t a t u a            7             4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_word</th>\n",
       "      <th>labels</th>\n",
       "      <th>previous_guesses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h e _ _ c l _ t _ s</td>\n",
       "      <td>h e r a c l i t u s</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_ e _ o d _ _ _</td>\n",
       "      <td>m e r o d a c h</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_ _ g _ _ _</td>\n",
       "      <td>i n g i r t</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_ _ _ z u o k a</td>\n",
       "      <td>s h i z u o k a</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m _ _ t _ _ _ _ n n e _ _ e d</td>\n",
       "      <td>m u l t i c h a n n e l l e d</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     masked_word                         labels  \\\n",
       "0            h e _ _ c l _ t _ s            h e r a c l i t u s   \n",
       "1                _ e _ o d _ _ _                m e r o d a c h   \n",
       "2                    _ _ g _ _ _                    i n g i r t   \n",
       "3                _ _ _ z u o k a                s h i z u o k a   \n",
       "4  m _ _ t _ _ _ _ n n e _ _ e d  m u l t i c h a n n e l l e d   \n",
       "\n",
       "                                    previous_guesses  \n",
       "0  [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...  \n",
       "4  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['word'],\n",
       "        num_rows: 204570\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['masked_word', 'labels', 'previous_guesses'],\n",
       "        num_rows: 22730\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['masked_word', 'labels', 'previous_guesses'],\n",
       "        num_rows: 130436\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather all the data in a DatasetDict\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train[[\"word\"]]),\n",
    "    \"valid\": Dataset.from_pandas(df_val),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling, BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataCollatorForMLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer, mlm_probability=None, max_length=42, prior_frequencies_path = None):\n",
    "        if mlm_probability is None:\n",
    "            mlm_probability = random.uniform(0.3, 0.8)\n",
    "        super().__init__(tokenizer=tokenizer, mlm_probability=mlm_probability)\n",
    "        self.max_length = max_length\n",
    "        self.char_to_index = {chr(i + ord('a')): i for i in range(26)}\n",
    "\n",
    "        self.prior_frequencies = None\n",
    "        # Load prior frequencies information\n",
    "        if prior_frequencies_path is not None:\n",
    "            self.prior_frequencies = torch.tensor(pd.read_csv(prior_frequencies_path).to_numpy(), dtype=torch.float32)\n",
    "            print(\"Loaded Prior Information: \", self.prior_frequencies.shape)\n",
    "        \n",
    "\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "\n",
    "        # Convert characters to their corresponding numerical labels\n",
    "        labels = []\n",
    "        words = []\n",
    "        \n",
    "        # Get a tensor of the shape (len(examples), 26) with the prior probabilities of each character\n",
    "        prior_probs_batch = torch.full((len(examples), 26), 1/26, dtype=torch.float32)\n",
    "\n",
    "        for i, example in enumerate(examples):\n",
    "            example = example['word']\n",
    "\n",
    "            # Adding prior information\n",
    "            if (self.prior_frequencies is not None) and len(example) <= self.prior_frequencies.shape[0]:\n",
    "                prior_probs_batch[i,:] = self.prior_frequencies[len(example)-1,:]\n",
    "\n",
    "            row_label = [self.char_to_index[char] for char in example.split()]\n",
    "            labels.append(row_label)\n",
    "            words.append(example)\n",
    "\n",
    "        # Pad labels to the same length (considering special tokens at the beginning and end of each label)\n",
    "        labels = [ [-100] + row_label + [-100] * (self.max_length - len(row_label) - 1) for row_label in labels]\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # Create a one-hot vector for the labels without consider the -100 values\n",
    "        prev_guess = torch.zeros((labels.shape[0], 26), dtype=torch.int64)\n",
    "        for i, label in enumerate(labels):\n",
    "            prev_guess[i][label[label != -100]] = 1\n",
    "\n",
    "            # NOTE: I could add until 5 random previous guesses to the prev_guess tensor\n",
    "            #  to account for maximum number of mistakes made when playing Hangman\n",
    "            random_guesses = torch.tensor([i for i in range(26) if i not in label])\n",
    "\n",
    "            # Take from 0 to 5 values from the random guesses randomly\n",
    "            if len(random_guesses) > 5:\n",
    "                random_guesses = random_guesses[torch.randperm(len(random_guesses))[:5]]\n",
    "                prev_guess[i][random_guesses] = 1\n",
    "\n",
    "        # Tokenize and pad the input examples\n",
    "        batch = self.tokenizer(words, truncation=True, padding='max_length', return_tensors=\"pt\", max_length=self.max_length)\n",
    "        \n",
    "        # Get the input_ids and apply masking\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        # labels = input_ids.clone()\n",
    "\n",
    "        for i, input_id in enumerate(input_ids):\n",
    "\n",
    "            # Get unique tokens\n",
    "            unique_tokens = torch.unique(input_id)\n",
    "            \n",
    "            # Filter Special Tokens by setting probability to 0.0\n",
    "            special_tokens_mask = self.tokenizer.get_special_tokens_mask(unique_tokens, already_has_special_tokens=True)\n",
    "            probabilities = torch.full(unique_tokens.shape, self.mlm_probability)\n",
    "            probabilities.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "            \n",
    "            # Filter the tokens to mask\n",
    "            tokens_to_mask = unique_tokens[torch.bernoulli(probabilities).bool()]\n",
    "\n",
    "            # Mask all instances of the chosen tokens\n",
    "            masked_indices = torch.zeros(input_id.shape, dtype=torch.bool)\n",
    "            for token in tokens_to_mask:\n",
    "                masked_indices[(input_id == token)] = True\n",
    "\n",
    "            labels[i][~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "            # 80% of the time, replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "            indices_replaced = torch.bernoulli(torch.full(input_id.shape, 0.8)).bool() & masked_indices\n",
    "            input_ids[i][indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "            # 10% of the time, replace masked input tokens with random word\n",
    "            # indices_random = torch.bernoulli(torch.full(input_id.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "            # random_words = torch.randint(26, input_id.shape, dtype=torch.long)\n",
    "            # input_ids[i][indices_random] = random_words[indices_random]\n",
    "\n",
    "            # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "\n",
    "        # Set to zero the values masked in the labels\n",
    "        for i, label in enumerate(labels):\n",
    "            prev_guess[i][label[label != -100]] = 0\n",
    "\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"prev_guess\"] = prev_guess\n",
    "        batch[\"prior_probs\"] = prior_probs_batch\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Prior Information:  torch.Size([28, 26])\n",
      "                                                word\n",
      "0  h e l l o h e l l o h e l l o h e l l o h e l ...\n",
      "1                                          w o r l d\n",
      "2                                          p a r i s\n",
      "3                                        b e r l i n\n",
      "Dataset({\n",
      "    features: ['word'],\n",
      "    num_rows: 4\n",
      "})\n",
      "tensor([[ 101, 1044, 1041, 1048, 1048,  103, 1044, 1041, 1048, 1048,  103, 1044,\n",
      "         1041, 1048, 1048,  103, 1044, 1041, 1048, 1048, 1051, 1044, 1041, 1048,\n",
      "         1048,  103, 1044, 1041, 1048, 1048,  103,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [ 101,  103,  103, 1054,  103, 1040,  102,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0]])\n",
      "tensor([[-100, -100, -100, -100, -100,   14, -100, -100, -100, -100,   14, -100,\n",
      "         -100, -100, -100,   14, -100, -100, -100, -100,   14, -100, -100, -100,\n",
      "         -100,   14, -100, -100, -100, -100,   14, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100],\n",
      "        [-100,   22,   14, -100,   11, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100]])\n",
      "tensor([[1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         1, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "         0, 1]])\n",
      "tensor([[0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385,\n",
      "         0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385,\n",
      "         0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385],\n",
      "        [0.0845, 0.0196, 0.0402, 0.0382, 0.1147, 0.0134, 0.0246, 0.0271, 0.0846,\n",
      "         0.0017, 0.0108, 0.0568, 0.0298, 0.0671, 0.0677, 0.0289, 0.0017, 0.0720,\n",
      "         0.0745, 0.0634, 0.0355, 0.0093, 0.0101, 0.0030, 0.0168, 0.0040]])\n"
     ]
    }
   ],
   "source": [
    "# TOY EXAMPLE\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the custom data collator\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer, \n",
    "                                         mlm_probability=0.5, \n",
    "                                         max_length=40, \n",
    "                                         prior_frequencies_path=\"data/total_rel_freq.csv\")\n",
    "\n",
    "# Toy dataset\n",
    "toy_data = {\n",
    "    \"word\": [\"h e l l o h e l l o h e l l o h e l l o h e l l o h e l l o\", \"w o r l d\", \"p a r i s\", \"b e r l i n\"]\n",
    "}\n",
    "df_temp = pd.DataFrame(toy_data)\n",
    "print(df_temp)\n",
    "\n",
    "# Tokenize the toy dataset\n",
    "TensorDataset = Dataset.from_pandas(df_temp)\n",
    "print(TensorDataset)\n",
    "toy_dataloader = DataLoader(TensorDataset, batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "# Print the first batch\n",
    "for batch in toy_dataloader:\n",
    "    print(batch['input_ids'])\n",
    "    print(batch['labels'])\n",
    "    print(batch['prev_guess'])\n",
    "    print(batch['prior_probs'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Prior Information:  torch.Size([28, 26])\n",
      "input_ids torch.int64\n",
      "labels torch.int64\n",
      "prev_guess torch.int64\n",
      "prior_probs torch.float32\n",
      "input_ids tensor([[ 101, 1056,  103, 1049, 1052, 1037,  103,  103,  102,    0],\n",
      "        [ 101, 1059,  103, 1054,  103, 1048,  103,  102,    0,    0],\n",
      "        [ 101, 1061,  103, 1050, 1055, 1056,  102,    0,    0,    0],\n",
      "        [ 101, 1043,  103, 1037,  103, 1043, 1041,  103, 1045, 1062],\n",
      "        [ 101, 1049, 1037,  103, 1037,  103, 1057, 1037,  102,    0],\n",
      "        [ 101, 1044, 1045, 1055, 1056, 1051, 1054, 1045,  103,  103],\n",
      "        [ 101, 1054,  103, 1039,  103, 1048, 1048,  103, 1039, 1056],\n",
      "        [ 101,  103,  103,  103, 1061, 1049, 1049,  103, 1040, 1045],\n",
      "        [ 101, 1055, 1056,  103,  103,  103,  102,    0,    0,    0],\n",
      "        [ 101, 1055, 1041, 1054, 1045,  103, 1051, 1039,  103, 1054]]) torch.Size([64, 42])\n",
      "labels tensor([[-100, -100,    8, -100,   15, -100,   13,    8, -100, -100],\n",
      "        [-100, -100,   14, -100,   18, -100,    4, -100, -100, -100],\n",
      "        [-100, -100,    8, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100,   17, -100,   13, -100, -100,   17, -100, -100],\n",
      "        [-100, -100, -100,   19, -100,   19, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100,   19, -100, -100, -100,    4,   19],\n",
      "        [-100, -100,    4, -100,   14, -100, -100,    4, -100, -100],\n",
      "        [-100,    2,   14,   18, -100, -100, -100,    4, -100, -100],\n",
      "        [-100, -100, -100,    8,   13,   10, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100,    2,   14,    2,    0, -100]]) torch.Size([64, 42])\n",
      "prev_guess tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0, 1, 1, 1, 0],\n",
      "        [1, 0, 0, 1, 0, 1, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 1, 1, 1, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 1, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 1, 0, 0, 0, 1, 0]]) torch.Size([64, 26])\n",
      "prior_probs tensor([[0.0772, 0.0153, 0.0469, 0.0310, 0.1066, 0.0107, 0.0228, 0.0263, 0.0965,\n",
      "         0.0009],\n",
      "        [0.0796, 0.0174, 0.0440, 0.0363, 0.1113, 0.0119, 0.0257, 0.0276, 0.0917,\n",
      "         0.0011],\n",
      "        [0.0845, 0.0196, 0.0402, 0.0382, 0.1147, 0.0134, 0.0246, 0.0271, 0.0846,\n",
      "         0.0017],\n",
      "        [0.0773, 0.0096, 0.0552, 0.0187, 0.0849, 0.0048, 0.0182, 0.0403, 0.0931,\n",
      "         0.0014],\n",
      "        [0.0772, 0.0153, 0.0469, 0.0310, 0.1066, 0.0107, 0.0228, 0.0263, 0.0965,\n",
      "         0.0009],\n",
      "        [0.0773, 0.0096, 0.0552, 0.0187, 0.0849, 0.0048, 0.0182, 0.0403, 0.0931,\n",
      "         0.0014],\n",
      "        [0.0661, 0.0086, 0.0632, 0.0489, 0.0920, 0.0144, 0.0115, 0.0316, 0.1121,\n",
      "         0.0029],\n",
      "        [0.0773, 0.0096, 0.0552, 0.0187, 0.0849, 0.0048, 0.0182, 0.0403, 0.0931,\n",
      "         0.0014],\n",
      "        [0.0845, 0.0196, 0.0402, 0.0382, 0.1147, 0.0134, 0.0246, 0.0271, 0.0846,\n",
      "         0.0017],\n",
      "        [0.0661, 0.0086, 0.0632, 0.0489, 0.0920, 0.0144, 0.0115, 0.0316, 0.1121,\n",
      "         0.0029]]) torch.Size([64, 26])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the custom data collator\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer, mlm_probability=0.5, max_length=42, prior_frequencies_path=\"data/total_rel_freq.csv\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    data['train'], \n",
    "    batch_size=64,\n",
    "    collate_fn = data_collator\n",
    ")\n",
    "\n",
    "# Prepare a batch using the custom data collator\n",
    "for batch in train_dataloader:\n",
    "    print(\"input_ids\", batch['input_ids'].dtype)\n",
    "    print(\"labels\", batch['labels'].dtype)\n",
    "    print(\"prev_guess\", batch['prev_guess'].dtype)\n",
    "    print(\"prior_probs\", batch['prior_probs'].dtype)\n",
    "\n",
    "    print(\"input_ids\", batch['input_ids'][:10,:10], batch['input_ids'].shape)\n",
    "    print(\"labels\", batch['labels'][:10,:10], batch['labels'].shape)\n",
    "    print(\"prev_guess\", batch['prev_guess'][:10,:10], batch['prev_guess'].shape)\n",
    "    print(\"prior_probs\", batch['prior_probs'][:10,:10], batch['prior_probs'].shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Prior Information:  torch.Size([28, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9/9 [00:00<00:00, 659.09 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'attention_mask', 'prev_guess', 'prior_probs'],\n",
      "    num_rows: 9\n",
      "})\n",
      "tensor([ 101, 1044, 1041,  103,  103, 1039, 1048,  103, 1056,  103, 1055,  102,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "tensor([-100, -100, -100,   17,    0, -100, -100,    8, -100,   20, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "torch.Size([9, 42])\n",
      "torch.Size([9, 42])\n",
      "torch.Size([9, 26])\n",
      "torch.Size([9, 26])\n",
      "input_ids torch.int64\n",
      "labels torch.int64\n",
      "prev_guess torch.int64\n",
      "prior_probs torch.float32\n",
      "input_ids tensor([ 101, 1044, 1041,  103,  103, 1039, 1048,  103, 1056,  103, 1055,  102,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0]) torch.Size([3, 42])\n",
      "labels tensor([-100, -100, -100,   17,    0, -100, -100,    8, -100,   20, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100]) torch.Size([3, 42])\n",
      "prev_guess tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0]) torch.Size([3, 26])\n",
      "prior_probs tensor([0.0800, 0.0187, 0.0413, 0.0387, 0.1154, 0.0131, 0.0254, 0.0271, 0.0887,\n",
      "        0.0013, 0.0083, 0.0576, 0.0294, 0.0709, 0.0693, 0.0303, 0.0017, 0.0719,\n",
      "        0.0666, 0.0653, 0.0356, 0.0100, 0.0088, 0.0029, 0.0181, 0.0038]) torch.Size([3, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class FixedTokenizer:\n",
    "    def __init__(self, tokenizer, max_length=42, prior_frequencies_path = None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.char_to_index = {chr(i + ord('a')): i for i in range(26)}\n",
    "\n",
    "        self.prior_frequencies = None\n",
    "        # Load prior frequencies information\n",
    "        if prior_frequencies_path is not None:\n",
    "            self.prior_frequencies = torch.tensor(pd.read_csv(prior_frequencies_path).to_numpy(), dtype=torch.float32)\n",
    "            print(\"Loaded Prior Information: \", self.prior_frequencies.shape)\n",
    "\n",
    "    def __call__(self, examples):\n",
    "\n",
    "        masked_word = examples['masked_word']\n",
    "        labels_word = examples['labels']\n",
    "        prev_guesses = examples['previous_guesses']\n",
    "\n",
    "        # Create a labels tensor with all the values to -100\n",
    "        labels_batch = torch.full((len(masked_word),self.max_length,), -100, dtype=torch.int64)\n",
    "\n",
    "        # Get a tensor of the shape (len(examples), 26) with the prior probabilities of each character\n",
    "        prior_probs_batch = torch.full((len(masked_word), 26), 1/26, dtype=torch.float32)\n",
    "\n",
    "        for i in range(len(masked_word)):\n",
    "\n",
    "\n",
    "            if (self.prior_frequencies is not None) and len(masked_word[i].split()) <= self.prior_frequencies.shape[0]:\n",
    "                prior_probs_batch[i,:] = self.prior_frequencies[len(masked_word[i].split())-1,:]\n",
    "\n",
    "            # Replace the underscore in masked_word with the special [MASK] token\n",
    "            masked_word[i] = masked_word[i].replace('_', '[MASK]')\n",
    "\n",
    "        # Tokenize the masked_words\n",
    "        batch = self.tokenizer(masked_word, truncation=True, padding='max_length', return_tensors=\"pt\", max_length=self.max_length)\n",
    "\n",
    "        # Create the labels per word\n",
    "        for i in range(len(masked_word)):\n",
    "\n",
    "            # Split labels_word[i] into a list of characters considering they are separated by spaces\n",
    "            labels_word[i] = labels_word[i].split()\n",
    "            masked_word[i] = masked_word[i].split()\n",
    "\n",
    "\n",
    "            # Convert labels_word to their corresponding numerical labels\n",
    "            for j, char in enumerate(labels_word[i]):\n",
    "                if masked_word[i][j] == '[MASK]':\n",
    "                    labels_batch[i][j + 1] = self.char_to_index[char]\n",
    "\n",
    "        prev_guesses_batch = torch.tensor(prev_guesses, dtype=torch.int64)\n",
    "\n",
    "        batch['prev_guess'] = prev_guesses_batch\n",
    "        batch['labels'] = labels_batch\n",
    "        batch['prior_probs'] = prior_probs_batch\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example usage with first 9 word\n",
    "df = df_val[:9]\n",
    "\n",
    "data_set = Dataset.from_pandas(df)\n",
    "\n",
    "# Initialize the data collator\n",
    "tokenizer = FixedTokenizer(tokenizer, max_length=42, prior_frequencies_path=\"data/total_rel_freq.csv\")\n",
    "\n",
    "# Process the examples\n",
    "data_set = data_set.map(tokenizer, batched=True, batch_size=3)\n",
    "data_set.set_format(\"torch\")\n",
    "data_set = data_set.remove_columns(['masked_word','previous_guesses', 'token_type_ids'])\n",
    "\n",
    "print(data_set)\n",
    "\n",
    "print(data_set['input_ids'][0])\n",
    "print(data_set['labels'][0])\n",
    "print(data_set['prev_guess'][0])\n",
    "\n",
    "# Print the shapes of the processed data\n",
    "print(data_set['input_ids'].shape)\n",
    "print(data_set['labels'].shape)\n",
    "print(data_set['prev_guess'].shape)\n",
    "print(data_set['prior_probs'].shape)\n",
    "\n",
    "# Data loader\n",
    "val_dataloader = DataLoader(\n",
    "    data_set, \n",
    "    batch_size=3,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Check the first batch of the validation data loader\n",
    "for batch in val_dataloader:\n",
    "    print(\"input_ids\", batch['input_ids'].dtype)\n",
    "    print(\"labels\", batch['labels'].dtype)\n",
    "    print(\"prev_guess\", batch['prev_guess'].dtype)\n",
    "    print(\"prior_probs\", batch['prior_probs'].dtype)\n",
    "\n",
    "    print(\"input_ids\", batch['input_ids'][0], batch['input_ids'].shape)\n",
    "    print(\"labels\", batch['labels'][0], batch['labels'].shape)\n",
    "    print(\"prev_guess\", batch['prev_guess'][0], batch['prev_guess'].shape)\n",
    "    print(\"prior_probs\", batch['prior_probs'][0], batch['prior_probs'].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Prior Information:  torch.Size([28, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22730/22730 [00:03<00:00, 6719.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Validation and Test datasets\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the data collator\n",
    "tokenizer = FixedTokenizer(tokenizer, max_length=42, prior_frequencies_path=\"data/total_rel_freq.csv\")\n",
    "\n",
    "\n",
    "# Tokenize the val dataset\n",
    "tokenized_val_data = data['valid'].map(tokenizer, batched=True)\n",
    "tokenized_val_data.set_format(\"torch\")\n",
    "tokenized_val_data = tokenized_val_data.remove_columns(['masked_word','previous_guesses', 'token_type_ids'])\n",
    "\n",
    "# Tokenize the test dataset\n",
    "# tokenized_test_data = data['test'].map(tokenizer, batched=True, batch_size=3)\n",
    "# tokenized_test_data.set_format(\"torch\")\n",
    "# tokenized_test_data = tokenized_test_data.remove_columns(['masked_word','previous_guesses', 'token_type_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desing the model for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset,DatasetDict\n",
    "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanNet(nn.Module):\n",
    "  def __init__(self,checkpoint, vocab_size = 26, hidden_ffn_size = 410, unfreeze_layers = 0, alpha = 0.5): \n",
    "    super(HangmanNet,self).__init__() \n",
    "    self.num_labels = vocab_size\n",
    "    self.alpha = alpha \n",
    "\n",
    "    #Load Model with given checkpoint and extract its body\n",
    "    self.model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "    \n",
    "    # Freeze all layers in the BERT model\n",
    "    for param in self.model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the last `unfreeze_layers` layers\n",
    "    if unfreeze_layers > 0:\n",
    "        for layer in self.model.encoder.layer[-unfreeze_layers:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Linear(768 + vocab_size, hidden_ffn_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_ffn_size, 26)\n",
    "    )\n",
    "    \n",
    "    # self.classifier = nn.Linear(768 + vocab_size,vocab_size) # load and initialize weights\n",
    "  \n",
    "  def forward(self, input_ids=None, attention_mask=None, labels=None, prev_guess=None,\n",
    "              token_type_ids=None, prior_probs=None):\n",
    "      outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "      \n",
    "      sequence_output = outputs.last_hidden_state  # (batch_size, sequence_length, hidden_size)\n",
    "      sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "      # Concatenate the previous guesses to the sequence_output\n",
    "      # (batch_size, sequence_length, hidden_size + vocab_size)\n",
    "      sequence_output = torch.cat((sequence_output, prev_guess.unsqueeze(1).repeat(1, sequence_output.shape[1], 1)), dim=2)\n",
    "      # sequence_output = torch.cat((sequence_output, prior_probs.unsqueeze(1).repeat(1, sequence_output.shape[1], 1)), dim=2)\n",
    "\n",
    "      logits = self.classifier(sequence_output)  # (batch_size, sequence_length, num_labels)\n",
    "\n",
    "      loss = None\n",
    "      if labels is not None:\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "          loss_aux_fct = nn.MSELoss()\n",
    "\n",
    "          # AUXILIARY LOSS\n",
    "          # Compute the auxiliary loss as the mse between the probabilities of active logits and the prior probabilities\n",
    "          # NOTE: softmax will lead the infinities to zero\n",
    "          mask_active_logits = labels.view(-1) != -100\n",
    "          active_logits = logits.view(-1, self.num_labels)[mask_active_logits]\n",
    "          prior_probs = prior_probs.unsqueeze(1).repeat(1, sequence_output.shape[1], 1)\n",
    "          active_prior_probs = prior_probs.view(-1, self.num_labels)[mask_active_logits]\n",
    "          \n",
    "          active_probs_model = torch.softmax(active_logits, dim=-1)\n",
    "          \n",
    "          # NOTE: I'm calculating the loss without masking the previous guesses\n",
    "          auxiliar_loss = loss_aux_fct(active_probs_model, active_prior_probs)\n",
    "\n",
    "          # MAIN CROSS ENTROPY LOSS\n",
    "          # Mask the logits to zero out probabilities of previously guessed characters by considering the one-hot encoding in prev guesses\n",
    "          mask_prev_guess = prev_guess.unsqueeze(1).repeat(1, sequence_output.shape[1], 1) == 1\n",
    "          logits[mask_prev_guess] = -float(\"inf\")\n",
    "\n",
    "          # NOTE: I don't need to mask logits or labels, because labels are already set with the value -100, which will be cancel by the loss function\n",
    "          masking_loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "\n",
    "          loss = masking_loss + self.alpha * auxiliar_loss\n",
    "        \n",
    "      return TokenClassifierOutput(logits=logits, loss=loss, hidden_states=outputs.hidden_states,attentions=outputs.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8497, grad_fn=<AddBackward0>)\n",
      "torch.Size([64, 42, 26])\n"
     ]
    }
   ],
   "source": [
    "# Check the model output\n",
    "device = \"cpu\"\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "model = HangmanNet(checkpoint=checkpoint, vocab_size = 26, unfreeze_layers = 1).to(device)\n",
    "\n",
    "# Prepare a batch using the custom data collator\n",
    "for batch in train_dataloader:\n",
    "\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "    prev_guess = batch['prev_guess']\n",
    "    prior_probs = batch['prior_probs']\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, \n",
    "                    attention_mask=attention_mask, \n",
    "                    labels=labels, \n",
    "                    prev_guess=prev_guess,\n",
    "                    prior_probs=prior_probs)\n",
    "    print(outputs.loss)\n",
    "    print(outputs.logits.shape)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "model.encoder.layer.10.attention.self.query.weight\n",
      "model.encoder.layer.10.attention.self.query.bias\n",
      "model.encoder.layer.10.attention.self.key.weight\n",
      "model.encoder.layer.10.attention.self.key.bias\n",
      "model.encoder.layer.10.attention.self.value.weight\n",
      "model.encoder.layer.10.attention.self.value.bias\n",
      "model.encoder.layer.10.attention.output.dense.weight\n",
      "model.encoder.layer.10.attention.output.dense.bias\n",
      "model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.10.intermediate.dense.weight\n",
      "model.encoder.layer.10.intermediate.dense.bias\n",
      "model.encoder.layer.10.output.dense.weight\n",
      "model.encoder.layer.10.output.dense.bias\n",
      "model.encoder.layer.10.output.LayerNorm.weight\n",
      "model.encoder.layer.10.output.LayerNorm.bias\n",
      "model.encoder.layer.11.attention.self.query.weight\n",
      "model.encoder.layer.11.attention.self.query.bias\n",
      "model.encoder.layer.11.attention.self.key.weight\n",
      "model.encoder.layer.11.attention.self.key.bias\n",
      "model.encoder.layer.11.attention.self.value.weight\n",
      "model.encoder.layer.11.attention.self.value.bias\n",
      "model.encoder.layer.11.attention.output.dense.weight\n",
      "model.encoder.layer.11.attention.output.dense.bias\n",
      "model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.11.intermediate.dense.weight\n",
      "model.encoder.layer.11.intermediate.dense.bias\n",
      "model.encoder.layer.11.output.dense.weight\n",
      "model.encoder.layer.11.output.dense.bias\n",
      "model.encoder.layer.11.output.LayerNorm.weight\n",
      "model.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.2.weight\n",
      "classifier.2.bias\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "unfreeze_layers = 2\n",
    "alpha = 0.5\n",
    "model = HangmanNet(checkpoint=checkpoint, vocab_size = 26, unfreeze_layers = unfreeze_layers, alpha=alpha).to(device)\n",
    "\n",
    "# Print the trainable parameters of the model\n",
    "print(\"Trainable parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "lr = 0.00005\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "num_epochs = 50\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     \"linear\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=num_training_steps,\n",
    "# )\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Prior Information:  torch.Size([28, 26])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "mlm_probability = 0.5\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer, \n",
    "                                        #  mlm_probability=mlm_probability, \n",
    "                                         max_length=42, \n",
    "                                         prior_frequencies_path=\"data/total_rel_freq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    data[\"train\"], \n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=True)\n",
    "\n",
    "# Probably add shuffle\n",
    "    \n",
    "eval_dataloader = DataLoader(tokenized_val_data,\n",
    "                             batch_size=64,\n",
    "                             num_workers=8,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new metric, which is more representative of how actually the network is playing Hangman\n",
    "def accuracy_unique_char(logits, labels):\n",
    "    batch_size, _, _ = logits.shape\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Extract the logits and labels for the current sequence\n",
    "        logits_seq = logits[i]\n",
    "        labels_seq = labels[i]\n",
    "\n",
    "        # Identify the indices of active tokens (not -100)\n",
    "        active_indices = labels_seq != -100\n",
    "\n",
    "        # Get the logits of the active tokens\n",
    "        active_logits = logits_seq[active_indices]\n",
    "\n",
    "        if active_logits.shape[0] == 0:\n",
    "            continue  # skip sequences with no active tokens\n",
    "\n",
    "        # Get the class with the highest probability among active tokens\n",
    "        max_prob_class = torch.argmax(active_logits, dim=-1)\n",
    "\n",
    "        # Get the actual labels of the active tokens\n",
    "        active_labels = labels_seq[active_indices].unique()\n",
    "\n",
    "        # Get the probabilities of the active tokens\n",
    "        max_prob = torch.max(active_logits, dim=-1).values\n",
    "\n",
    "        # Get the index of the maximum probability\n",
    "        max_prob_index = torch.argmax(max_prob)\n",
    "\n",
    "        # Take a greedy choose and take the largest probability\n",
    "        max_prob_char = max_prob_class[max_prob_index]\n",
    "\n",
    "        if max_prob_char in active_labels:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    return correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: models/prior_prob_models/model_lr_5e-05_mlm_prob_0.5_alpha_0.5_unfreeze_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50: 100%|██████████| 3197/3197 [05:21<00:00,  9.94it/s]\n",
      "Validation Epoch 1/50: 100%|██████████| 356/356 [00:41<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Training Loss: 1.7738\n",
      "Validation Loss: 2.1317\n",
      "Validation Accuracy: 0.2964\n",
      "Validation F1 Score: 0.2021\n",
      "Validation Accuracy Hangman: 0.5658\n",
      "Learning Rate: 0.00005000\n",
      "Loss Change: 998.2262 (train), 997.8683 (val)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/50:   7%|▋         | 225/3197 [00:26<05:51,  8.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     49\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 50\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     52\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 55\u001b[0m, in \u001b[0;36mHangmanNet.forward\u001b[0;34m(self, input_ids, attention_mask, labels, prev_guess, token_type_ids, prior_probs)\u001b[0m\n\u001b[1;32m     53\u001b[0m mask_active_logits \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     54\u001b[0m active_logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels)[mask_active_logits]\n\u001b[0;32m---> 55\u001b[0m prior_probs \u001b[38;5;241m=\u001b[39m \u001b[43mprior_probs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, sequence_output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     56\u001b[0m active_prior_probs \u001b[38;5;241m=\u001b[39m prior_probs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels)[mask_active_logits]\n\u001b[1;32m     58\u001b[0m active_probs_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(active_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "save_dir = \"models/prior_prob_models\"\n",
    "save_dir = os.path.join(save_dir, f\"model_lr_{lr}_mlm_prob_{mlm_probability}_alpha_{alpha}_unfreeze_{unfreeze_layers}\")\n",
    "print(f\"Saving model to: {save_dir}\")\n",
    "writer = SummaryWriter(log_dir=save_dir)\n",
    "\n",
    "# Initialize metrics storage\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_f1_scores = []\n",
    "val_accuracies_hangman = []\n",
    "\n",
    "# Define the save frequency\n",
    "save_frequency = 5  # Save model every n epochs, adjust this as needed\n",
    "\n",
    "# Load the model checkpoint if available\n",
    "checkpoint_path = \"path_to_checkpoint\"  # Provide the path to your saved checkpoint\n",
    "start_epoch = 0\n",
    "\n",
    "prev_val_loss = 1000\n",
    "prev_train_loss = 1000\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    val_accuracies = checkpoint['val_accuracies']\n",
    "    val_f1_scores = checkpoint['val_f1_scores']\n",
    "    val_accuracies_hangman = checkpoint['val_accuracies_hangman']\n",
    "    print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        # lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    # Intialize to calculate our new metric\n",
    "    correct_predictions = 0\n",
    "    total_sequences = 0\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        flat_predictions = predictions.view(-1)\n",
    "        flat_references = batch[\"labels\"].view(-1)\n",
    "        flat_attention_mask = batch[\"labels\"].view(-1) != -100\n",
    "\n",
    "        active_predictions = flat_predictions[flat_attention_mask]\n",
    "        active_references = flat_references[flat_attention_mask]\n",
    "\n",
    "        all_predictions.extend(active_predictions.cpu().numpy())\n",
    "        all_references.extend(active_references.cpu().numpy())\n",
    "\n",
    "        # Calculate the custom accuracy\n",
    "        accuracy_hangman = accuracy_unique_char(logits, batch[\"labels\"])\n",
    "        correct_predictions += accuracy_hangman\n",
    "        total_sequences += logits.shape[0]\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(eval_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    val_accuracy = accuracy_score(all_references, all_predictions)\n",
    "    val_f1 = f1_score(all_references, all_predictions, average=\"macro\")\n",
    "    val_accuracy_hangman = correct_predictions / total_sequences\n",
    "\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_f1_scores.append(val_f1)\n",
    "    val_accuracies_hangman.append(val_accuracy_hangman)\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalars('Loss', {'train_loss': avg_train_loss, 'val_loss' : avg_val_loss}, epoch)\n",
    "    writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "    writer.add_scalar('Accuracy/Validation_Hangman', val_accuracy_hangman, epoch)\n",
    "    writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "    writer.add_scalars('Loss Change: ', {'train_loss_change': prev_train_loss - avg_train_loss, \n",
    "                                        'val_loss_change': prev_val_loss - avg_val_loss}, epoch)\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "    print(f\"Validation Accuracy Hangman: {val_accuracy_hangman:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.8f}\")\n",
    "    print(f\"Loss Change: {prev_train_loss - avg_train_loss:.4f} (train), {prev_val_loss - avg_val_loss:.4f} (val)\")\n",
    "\n",
    "    prev_train_loss = avg_train_loss\n",
    "    prev_val_loss = avg_val_loss\n",
    "    \n",
    "    # Save the model every n epochs\n",
    "    if (epoch + 1) % save_frequency == 0:\n",
    "        model_save_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pth\")\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'val_f1_scores': val_f1_scores,\n",
    "            'val_accuracies_hangman': val_accuracies_hangman\n",
    "        }\n",
    "        torch.save(checkpoint, model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save the final model\n",
    "model_save_path = os.path.join(save_dir, \"model_final.pth\")\n",
    "checkpoint = {\n",
    "    'epoch': epoch + 1,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'val_f1_scores': val_f1_scores,\n",
    "    'val_accuracies_hangman': val_accuracies_hangman\n",
    "}\n",
    "torch.save(checkpoint, model_save_path)\n",
    "print(f\"Final model saved to {model_save_path}\")\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "character-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
