{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs\n",
    "# _ _ _ _ _\n",
    "\n",
    "# 250000 words\n",
    "\n",
    "# El potter no [MASK] novia\n",
    "\n",
    "# p _ t t e r\n",
    "\n",
    "\n",
    "# 1. Filtrar las palabras\n",
    "# 2. Palabras mas comunes\n",
    "# 3. Data colador\n",
    "# 4. Penalize more small words??\n",
    "# 5. Podria concatenar las palabras adivinadas hasta ahora al final del classifier, es practicamente concatenar un one-hot vector \n",
    "# de 0-26 con 1 donde las palabras ya fueron adivinadas.\n",
    "# 6. Podria usar one-hot encodings 0-27 para los characteres\n",
    "# 7. El Test y el val tambien generalo de manera dinamica. No te preocupes por que distribucion tendra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict, Dataset\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Set a seed for all libraries\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word  word_length  unique_chars\n",
      "0      timpani            7             6\n",
      "1       worsle            6             6\n",
      "2        yinst            5             5\n",
      "3  grangerized           11             8\n",
      "4      matatua            7             4\n",
      "(204570, 3)\n",
      "(22730, 3)\n",
      "(130436, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets \n",
    "df_train = pd.read_csv('data/to_mask_train.csv', converters={'word': str})\n",
    "df_val = pd.read_csv('data/masked_val.csv', converters={'masked_word': str, 'labels': str, 'previous_guesses': eval})\n",
    "df_test = pd.read_csv('data/masked_test.csv', converters={'masked_word': str, 'labels': str, 'previous_guesses': eval})\n",
    "\n",
    "print(df_train.head())\n",
    "\n",
    "# Add spaces to the word in training\n",
    "df_train[\"word\"] = df_train[\"word\"].apply(lambda x: ' '.join(list(x)))\n",
    "\n",
    "# Add spaces to the masked_word and labels\n",
    "df_val[\"masked_word\"] = df_val[\"masked_word\"].apply(lambda x: ' '.join(list(x)))\n",
    "df_val[\"labels\"] = df_val[\"labels\"].apply(lambda x: ' '.join(list(x)))\n",
    "df_test[\"masked_word\"] = df_test[\"masked_word\"].apply(lambda x: ' '.join(list(x)))\n",
    "df_test[\"labels\"] = df_test[\"labels\"].apply(lambda x: ' '.join(list(x)))\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>word_length</th>\n",
       "      <th>unique_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t i m p a n i</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>w o r s l e</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>y i n s t</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g r a n g e r i z e d</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m a t a t u a</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    word  word_length  unique_chars\n",
       "0          t i m p a n i            7             6\n",
       "1            w o r s l e            6             6\n",
       "2              y i n s t            5             5\n",
       "3  g r a n g e r i z e d           11             8\n",
       "4          m a t a t u a            7             4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_word</th>\n",
       "      <th>labels</th>\n",
       "      <th>previous_guesses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h e _ _ c l _ t _ s</td>\n",
       "      <td>h e r a c l i t u s</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_ e _ o d _ _ _</td>\n",
       "      <td>m e r o d a c h</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_ _ g _ _ _</td>\n",
       "      <td>i n g i r t</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_ _ _ z u o k a</td>\n",
       "      <td>s h i z u o k a</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m _ _ t _ _ _ _ n n e _ _ e d</td>\n",
       "      <td>m u l t i c h a n n e l l e d</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     masked_word                         labels  \\\n",
       "0            h e _ _ c l _ t _ s            h e r a c l i t u s   \n",
       "1                _ e _ o d _ _ _                m e r o d a c h   \n",
       "2                    _ _ g _ _ _                    i n g i r t   \n",
       "3                _ _ _ z u o k a                s h i z u o k a   \n",
       "4  m _ _ t _ _ _ _ n n e _ _ e d  m u l t i c h a n n e l l e d   \n",
       "\n",
       "                                    previous_guesses  \n",
       "0  [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...  \n",
       "4  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['word'],\n",
       "        num_rows: 204570\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['masked_word', 'labels', 'previous_guesses'],\n",
       "        num_rows: 22730\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['masked_word', 'labels', 'previous_guesses'],\n",
       "        num_rows: 130436\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather all the data in a DatasetDict\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train[[\"word\"]]),\n",
    "    \"valid\": Dataset.from_pandas(df_val),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling, BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataCollatorForMLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer, mlm_probability=None, max_length=42, prior_frequencies_path = None):\n",
    "        if mlm_probability is None:\n",
    "            mlm_probability = random.uniform(0.3, 0.8)\n",
    "        super().__init__(tokenizer=tokenizer, mlm_probability=mlm_probability)\n",
    "        self.max_length = max_length\n",
    "        self.char_to_index = {chr(i + ord('a')): i for i in range(26)}\n",
    "\n",
    "        self.prior_frequencies = None\n",
    "        # Load prior frequencies information\n",
    "        if prior_frequencies_path is not None:\n",
    "            self.prior_frequencies = torch.tensor(pd.read_csv(prior_frequencies_path).to_numpy(), dtype=torch.float32)\n",
    "            print(\"Loaded Prior Information: \", self.prior_frequencies.shape)\n",
    "        \n",
    "\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "\n",
    "        # Convert characters to their corresponding numerical labels\n",
    "        labels = []\n",
    "        words = []\n",
    "        \n",
    "        # Get a tensor of the shape (len(examples), 26) with the prior probabilities of each character\n",
    "        prior_probs_batch = torch.full((len(examples), 26), 1/26, dtype=torch.float32)\n",
    "\n",
    "        for i, example in enumerate(examples):\n",
    "            example = example['word']\n",
    "\n",
    "            # Adding prior information\n",
    "            if (self.prior_frequencies is not None) and len(example) <= self.prior_frequencies.shape[0]:\n",
    "                prior_probs_batch[i,:] = self.prior_frequencies[len(example)-1,:]\n",
    "\n",
    "            row_label = [self.char_to_index[char] for char in example.split()]\n",
    "            labels.append(row_label)\n",
    "            words.append(example)\n",
    "\n",
    "        # Pad labels to the same length (considering special tokens at the beginning and end of each label)\n",
    "        labels = [ [-100] + row_label + [-100] * (self.max_length - len(row_label) - 1) for row_label in labels]\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # Create a one-hot vector for the labels without consider the -100 values\n",
    "        prev_guess = torch.zeros((labels.shape[0], 26), dtype=torch.int64)\n",
    "        for i, label in enumerate(labels):\n",
    "            prev_guess[i][label[label != -100]] = 1\n",
    "\n",
    "            # NOTE: I could add until 5 random previous guesses to the prev_guess tensor\n",
    "            #  to account for maximum number of mistakes made when playing Hangman\n",
    "            random_guesses = torch.tensor([i for i in range(26) if i not in label])\n",
    "\n",
    "            # Take from 0 to 5 values from the random guesses randomly\n",
    "            if len(random_guesses) > 5:\n",
    "                random_guesses = random_guesses[torch.randperm(len(random_guesses))[:5]]\n",
    "                prev_guess[i][random_guesses] = 1\n",
    "\n",
    "        # Tokenize and pad the input examples\n",
    "        batch = self.tokenizer(words, truncation=True, padding='max_length', return_tensors=\"pt\", max_length=self.max_length)\n",
    "        \n",
    "        # Get the input_ids and apply masking\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        # labels = input_ids.clone()\n",
    "\n",
    "        for i, input_id in enumerate(input_ids):\n",
    "\n",
    "            # Get unique tokens\n",
    "            unique_tokens = torch.unique(input_id)\n",
    "            \n",
    "            # Filter Special Tokens by setting probability to 0.0\n",
    "            special_tokens_mask = self.tokenizer.get_special_tokens_mask(unique_tokens, already_has_special_tokens=True)\n",
    "            probabilities = torch.full(unique_tokens.shape, self.mlm_probability)\n",
    "            probabilities.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "            \n",
    "            # Filter the tokens to mask\n",
    "            tokens_to_mask = unique_tokens[torch.bernoulli(probabilities).bool()]\n",
    "\n",
    "            # Mask all instances of the chosen tokens\n",
    "            masked_indices = torch.zeros(input_id.shape, dtype=torch.bool)\n",
    "            for token in tokens_to_mask:\n",
    "                masked_indices[(input_id == token)] = True\n",
    "\n",
    "            labels[i][~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "            # 80% of the time, replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "            indices_replaced = torch.bernoulli(torch.full(input_id.shape, 0.8)).bool() & masked_indices\n",
    "            input_ids[i][indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "            # 10% of the time, replace masked input tokens with random word\n",
    "            # indices_random = torch.bernoulli(torch.full(input_id.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "            # random_words = torch.randint(26, input_id.shape, dtype=torch.long)\n",
    "            # input_ids[i][indices_random] = random_words[indices_random]\n",
    "\n",
    "            # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "\n",
    "        # Set to zero the values masked in the labels\n",
    "        for i, label in enumerate(labels):\n",
    "            prev_guess[i][label[label != -100]] = 0\n",
    "\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"prev_guess\"] = prev_guess\n",
    "        batch[\"prior_probs\"] = prior_probs_batch\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Prior Information:  torch.Size([28, 26])\n",
      "                                                word\n",
      "0  h e l l o h e l l o h e l l o h e l l o h e l ...\n",
      "1                                          w o r l d\n",
      "2                                          p a r i s\n",
      "3                                        b e r l i n\n",
      "Dataset({\n",
      "    features: ['word'],\n",
      "    num_rows: 4\n",
      "})\n",
      "tensor([[ 101, 1044, 1041, 1048, 1048,  103, 1044, 1041, 1048, 1048,  103, 1044,\n",
      "         1041, 1048, 1048,  103, 1044, 1041, 1048, 1048, 1051, 1044, 1041, 1048,\n",
      "         1048,  103, 1044, 1041, 1048, 1048,  103,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [ 101,  103,  103, 1054,  103, 1040,  102,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0]])\n",
      "tensor([[-100, -100, -100, -100, -100,   14, -100, -100, -100, -100,   14, -100,\n",
      "         -100, -100, -100,   14, -100, -100, -100, -100,   14, -100, -100, -100,\n",
      "         -100,   14, -100, -100, -100, -100,   14, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100],\n",
      "        [-100,   22,   14, -100,   11, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100]])\n",
      "tensor([[1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         1, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "         0, 1]])\n",
      "tensor([[0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385,\n",
      "         0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385,\n",
      "         0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385],\n",
      "        [0.0845, 0.0196, 0.0402, 0.0382, 0.1147, 0.0134, 0.0246, 0.0271, 0.0846,\n",
      "         0.0017, 0.0108, 0.0568, 0.0298, 0.0671, 0.0677, 0.0289, 0.0017, 0.0720,\n",
      "         0.0745, 0.0634, 0.0355, 0.0093, 0.0101, 0.0030, 0.0168, 0.0040]])\n"
     ]
    }
   ],
   "source": [
    "# TOY EXAMPLE\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the custom data collator\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer, \n",
    "                                         mlm_probability=0.5, \n",
    "                                         max_length=40, \n",
    "                                         prior_frequencies_path=\"data/total_rel_freq.csv\")\n",
    "\n",
    "# Toy dataset\n",
    "toy_data = {\n",
    "    \"word\": [\"h e l l o h e l l o h e l l o h e l l o h e l l o h e l l o\", \"w o r l d\", \"p a r i s\", \"b e r l i n\"]\n",
    "}\n",
    "df_temp = pd.DataFrame(toy_data)\n",
    "print(df_temp)\n",
    "\n",
    "# Tokenize the toy dataset\n",
    "TensorDataset = Dataset.from_pandas(df_temp)\n",
    "print(TensorDataset)\n",
    "toy_dataloader = DataLoader(TensorDataset, batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "# Print the first batch\n",
    "for batch in toy_dataloader:\n",
    "    print(batch['input_ids'])\n",
    "    print(batch['labels'])\n",
    "    print(batch['prev_guess'])\n",
    "    print(batch['prior_probs'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Prior Information:  torch.Size([28, 26])\n",
      "input_ids torch.int64\n",
      "labels torch.int64\n",
      "prev_guess torch.int64\n",
      "prior_probs torch.float32\n",
      "input_ids tensor([[ 101, 1056,  103, 1049, 1052, 1037,  103,  103,  102,    0],\n",
      "        [ 101, 1059,  103, 1054,  103, 1048,  103,  102,    0,    0],\n",
      "        [ 101, 1061,  103, 1050, 1055, 1056,  102,    0,    0,    0],\n",
      "        [ 101, 1043,  103, 1037,  103, 1043, 1041,  103, 1045, 1062],\n",
      "        [ 101, 1049, 1037,  103, 1037,  103, 1057, 1037,  102,    0],\n",
      "        [ 101, 1044, 1045, 1055, 1056, 1051, 1054, 1045,  103,  103],\n",
      "        [ 101, 1054,  103, 1039,  103, 1048, 1048,  103, 1039, 1056],\n",
      "        [ 101,  103,  103,  103, 1061, 1049, 1049,  103, 1040, 1045],\n",
      "        [ 101, 1055, 1056,  103,  103,  103,  102,    0,    0,    0],\n",
      "        [ 101, 1055, 1041, 1054, 1045,  103, 1051, 1039,  103, 1054]]) torch.Size([64, 42])\n",
      "labels tensor([[-100, -100,    8, -100,   15, -100,   13,    8, -100, -100],\n",
      "        [-100, -100,   14, -100,   18, -100,    4, -100, -100, -100],\n",
      "        [-100, -100,    8, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100,   17, -100,   13, -100, -100,   17, -100, -100],\n",
      "        [-100, -100, -100,   19, -100,   19, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100,   19, -100, -100, -100,    4,   19],\n",
      "        [-100, -100,    4, -100,   14, -100, -100,    4, -100, -100],\n",
      "        [-100,    2,   14,   18, -100, -100, -100,    4, -100, -100],\n",
      "        [-100, -100, -100,    8,   13,   10, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100,    2,   14,    2,    0, -100]]) torch.Size([64, 42])\n",
      "prev_guess tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0, 1, 1, 1, 0],\n",
      "        [1, 0, 0, 1, 0, 1, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 1, 1, 1, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 1, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 1, 0, 0, 0, 1, 0]]) torch.Size([64, 26])\n",
      "prior_probs tensor([[0.0772, 0.0153, 0.0469, 0.0310, 0.1066, 0.0107, 0.0228, 0.0263, 0.0965,\n",
      "         0.0009],\n",
      "        [0.0796, 0.0174, 0.0440, 0.0363, 0.1113, 0.0119, 0.0257, 0.0276, 0.0917,\n",
      "         0.0011],\n",
      "        [0.0845, 0.0196, 0.0402, 0.0382, 0.1147, 0.0134, 0.0246, 0.0271, 0.0846,\n",
      "         0.0017],\n",
      "        [0.0773, 0.0096, 0.0552, 0.0187, 0.0849, 0.0048, 0.0182, 0.0403, 0.0931,\n",
      "         0.0014],\n",
      "        [0.0772, 0.0153, 0.0469, 0.0310, 0.1066, 0.0107, 0.0228, 0.0263, 0.0965,\n",
      "         0.0009],\n",
      "        [0.0773, 0.0096, 0.0552, 0.0187, 0.0849, 0.0048, 0.0182, 0.0403, 0.0931,\n",
      "         0.0014],\n",
      "        [0.0661, 0.0086, 0.0632, 0.0489, 0.0920, 0.0144, 0.0115, 0.0316, 0.1121,\n",
      "         0.0029],\n",
      "        [0.0773, 0.0096, 0.0552, 0.0187, 0.0849, 0.0048, 0.0182, 0.0403, 0.0931,\n",
      "         0.0014],\n",
      "        [0.0845, 0.0196, 0.0402, 0.0382, 0.1147, 0.0134, 0.0246, 0.0271, 0.0846,\n",
      "         0.0017],\n",
      "        [0.0661, 0.0086, 0.0632, 0.0489, 0.0920, 0.0144, 0.0115, 0.0316, 0.1121,\n",
      "         0.0029]]) torch.Size([64, 26])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the custom data collator\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer, mlm_probability=0.5, max_length=42, prior_frequencies_path=\"data/total_rel_freq.csv\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    data['train'], \n",
    "    batch_size=64,\n",
    "    collate_fn = data_collator\n",
    ")\n",
    "\n",
    "# Prepare a batch using the custom data collator\n",
    "for batch in train_dataloader:\n",
    "    print(\"input_ids\", batch['input_ids'].dtype)\n",
    "    print(\"labels\", batch['labels'].dtype)\n",
    "    print(\"prev_guess\", batch['prev_guess'].dtype)\n",
    "    print(\"prior_probs\", batch['prior_probs'].dtype)\n",
    "\n",
    "    print(\"input_ids\", batch['input_ids'][:10,:10], batch['input_ids'].shape)\n",
    "    print(\"labels\", batch['labels'][:10,:10], batch['labels'].shape)\n",
    "    print(\"prev_guess\", batch['prev_guess'][:10,:10], batch['prev_guess'].shape)\n",
    "    print(\"prior_probs\", batch['prior_probs'][:10,:10], batch['prior_probs'].shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Prior Information:  torch.Size([28, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9/9 [00:00<00:00, 1076.41 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'attention_mask', 'prev_guess', 'prior_probs'],\n",
      "    num_rows: 9\n",
      "})\n",
      "tensor([ 101, 1044, 1041,  103,  103, 1039, 1048,  103, 1056,  103, 1055,  102,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "tensor([-100, -100, -100,   17,    0, -100, -100,    8, -100,   20, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "torch.Size([9, 42])\n",
      "torch.Size([9, 42])\n",
      "torch.Size([9, 26])\n",
      "torch.Size([9, 26])\n",
      "input_ids torch.int64\n",
      "labels torch.int64\n",
      "prev_guess torch.int64\n",
      "prior_probs torch.float32\n",
      "input_ids tensor([ 101, 1044, 1041,  103,  103, 1039, 1048,  103, 1056,  103, 1055,  102,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0]) torch.Size([3, 42])\n",
      "labels tensor([-100, -100, -100,   17,    0, -100, -100,    8, -100,   20, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100]) torch.Size([3, 42])\n",
      "prev_guess tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0]) torch.Size([3, 26])\n",
      "prior_probs tensor([0.0800, 0.0187, 0.0413, 0.0387, 0.1154, 0.0131, 0.0254, 0.0271, 0.0887,\n",
      "        0.0013, 0.0083, 0.0576, 0.0294, 0.0709, 0.0693, 0.0303, 0.0017, 0.0719,\n",
      "        0.0666, 0.0653, 0.0356, 0.0100, 0.0088, 0.0029, 0.0181, 0.0038]) torch.Size([3, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class FixedTokenizer:\n",
    "    def __init__(self, tokenizer, max_length=42, prior_frequencies_path = None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.char_to_index = {chr(i + ord('a')): i for i in range(26)}\n",
    "\n",
    "        self.prior_frequencies = None\n",
    "        # Load prior frequencies information\n",
    "        if prior_frequencies_path is not None:\n",
    "            self.prior_frequencies = torch.tensor(pd.read_csv(prior_frequencies_path).to_numpy(), dtype=torch.float32)\n",
    "            print(\"Loaded Prior Information: \", self.prior_frequencies.shape)\n",
    "\n",
    "    def __call__(self, examples):\n",
    "\n",
    "        masked_word = examples['masked_word']\n",
    "        labels_word = examples['labels']\n",
    "        prev_guesses = examples['previous_guesses']\n",
    "\n",
    "        # Create a labels tensor with all the values to -100\n",
    "        labels_batch = torch.full((len(masked_word),self.max_length,), -100, dtype=torch.int64)\n",
    "\n",
    "        # Get a tensor of the shape (len(examples), 26) with the prior probabilities of each character\n",
    "        prior_probs_batch = torch.full((len(masked_word), 26), 1/26, dtype=torch.float32)\n",
    "\n",
    "        for i in range(len(masked_word)):\n",
    "\n",
    "\n",
    "            if (self.prior_frequencies is not None) and len(masked_word[i].split()) <= self.prior_frequencies.shape[0]:\n",
    "                prior_probs_batch[i,:] = self.prior_frequencies[len(masked_word[i].split())-1,:]\n",
    "\n",
    "            # Replace the underscore in masked_word with the special [MASK] token\n",
    "            masked_word[i] = masked_word[i].replace('_', '[MASK]')\n",
    "\n",
    "        # Tokenize the masked_words\n",
    "        batch = self.tokenizer(masked_word, truncation=True, padding='max_length', return_tensors=\"pt\", max_length=self.max_length)\n",
    "\n",
    "        # Create the labels per word\n",
    "        for i in range(len(masked_word)):\n",
    "\n",
    "            # Split labels_word[i] into a list of characters considering they are separated by spaces\n",
    "            labels_word[i] = labels_word[i].split()\n",
    "            masked_word[i] = masked_word[i].split()\n",
    "\n",
    "\n",
    "            # Convert labels_word to their corresponding numerical labels\n",
    "            for j, char in enumerate(labels_word[i]):\n",
    "                if masked_word[i][j] == '[MASK]':\n",
    "                    labels_batch[i][j + 1] = self.char_to_index[char]\n",
    "\n",
    "        prev_guesses_batch = torch.tensor(prev_guesses, dtype=torch.int64)\n",
    "\n",
    "        batch['prev_guess'] = prev_guesses_batch\n",
    "        batch['labels'] = labels_batch\n",
    "        batch['prior_probs'] = prior_probs_batch\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example usage with first 9 word\n",
    "df = df_val[:9]\n",
    "\n",
    "data_set = Dataset.from_pandas(df)\n",
    "\n",
    "# Initialize the data collator\n",
    "tokenizer = FixedTokenizer(tokenizer, max_length=42, prior_frequencies_path=\"data/total_rel_freq.csv\")\n",
    "\n",
    "# Process the examples\n",
    "data_set = data_set.map(tokenizer, batched=True, batch_size=3)\n",
    "data_set.set_format(\"torch\")\n",
    "data_set = data_set.remove_columns(['masked_word','previous_guesses', 'token_type_ids'])\n",
    "\n",
    "print(data_set)\n",
    "\n",
    "print(data_set['input_ids'][0])\n",
    "print(data_set['labels'][0])\n",
    "print(data_set['prev_guess'][0])\n",
    "\n",
    "# Print the shapes of the processed data\n",
    "print(data_set['input_ids'].shape)\n",
    "print(data_set['labels'].shape)\n",
    "print(data_set['prev_guess'].shape)\n",
    "print(data_set['prior_probs'].shape)\n",
    "\n",
    "# Data loader\n",
    "val_dataloader = DataLoader(\n",
    "    data_set, \n",
    "    batch_size=3,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Check the first batch of the validation data loader\n",
    "for batch in val_dataloader:\n",
    "    print(\"input_ids\", batch['input_ids'].dtype)\n",
    "    print(\"labels\", batch['labels'].dtype)\n",
    "    print(\"prev_guess\", batch['prev_guess'].dtype)\n",
    "    print(\"prior_probs\", batch['prior_probs'].dtype)\n",
    "\n",
    "    print(\"input_ids\", batch['input_ids'][0], batch['input_ids'].shape)\n",
    "    print(\"labels\", batch['labels'][0], batch['labels'].shape)\n",
    "    print(\"prev_guess\", batch['prev_guess'][0], batch['prev_guess'].shape)\n",
    "    print(\"prior_probs\", batch['prior_probs'][0], batch['prior_probs'].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Prior Information:  torch.Size([28, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22730/22730 [00:03<00:00, 7457.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Validation and Test datasets\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the data collator\n",
    "tokenizer = FixedTokenizer(tokenizer, max_length=42, prior_frequencies_path=\"data/total_rel_freq.csv\")\n",
    "\n",
    "\n",
    "# Tokenize the val dataset\n",
    "tokenized_val_data = data['valid'].map(tokenizer, batched=True)\n",
    "tokenized_val_data.set_format(\"torch\")\n",
    "tokenized_val_data = tokenized_val_data.remove_columns(['masked_word','previous_guesses', 'token_type_ids'])\n",
    "\n",
    "# Tokenize the test dataset\n",
    "# tokenized_test_data = data['test'].map(tokenizer, batched=True, batch_size=3)\n",
    "# tokenized_test_data.set_format(\"torch\")\n",
    "# tokenized_test_data = tokenized_test_data.remove_columns(['masked_word','previous_guesses', 'token_type_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desing the model for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset,DatasetDict\n",
    "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanNet(nn.Module):\n",
    "  def __init__(self,checkpoint, vocab_size = 26, hidden_ffn_size = 410, unfreeze_layers = 0): \n",
    "    super(HangmanNet,self).__init__() \n",
    "    self.num_labels = vocab_size \n",
    "\n",
    "    #Load Model with given checkpoint and extract its body\n",
    "    self.model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "    \n",
    "    # Freeze all layers in the BERT model\n",
    "    for param in self.model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the last `unfreeze_layers` layers\n",
    "    if unfreeze_layers > 0:\n",
    "        for layer in self.model.encoder.layer[-unfreeze_layers:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Linear(768 + 2*vocab_size, hidden_ffn_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_ffn_size, 26)\n",
    "    )\n",
    "    \n",
    "    # self.classifier = nn.Linear(768 + vocab_size,vocab_size) # load and initialize weights\n",
    "  \n",
    "  def forward(self, input_ids=None, attention_mask=None, labels=None, prev_guess=None,\n",
    "              token_type_ids=None, prior_probs=None):\n",
    "      outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "      \n",
    "      sequence_output = outputs.last_hidden_state  # (batch_size, sequence_length, hidden_size)\n",
    "      sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "      # Concatenate the previous guesses to the sequence_output\n",
    "      # (batch_size, sequence_length, hidden_size + vocab_size)\n",
    "      sequence_output = torch.cat((sequence_output, prev_guess.unsqueeze(1).repeat(1, sequence_output.shape[1], 1)), dim=2)\n",
    "      sequence_output = torch.cat((sequence_output, prior_probs.unsqueeze(1).repeat(1, sequence_output.shape[1], 1)), dim=2)\n",
    "\n",
    "      logits = self.classifier(sequence_output)  # (batch_size, sequence_length, num_labels)\n",
    "\n",
    "      loss = None\n",
    "      if labels is not None:\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "          # NOTE: I already has the labels in active logits representation\n",
    "          # logits = logits + prior_probs.unsqueeze(1).repeat(1, sequence_output.shape[1], 1)\n",
    "\n",
    "          # Mask the logits to zero out probabilities of previously guessed characters by considering the one-hot encoding in prev guesses\n",
    "          logits[prev_guess.unsqueeze(1).repeat(1, sequence_output.shape[1], 1) == 1] = -float(\"inf\")\n",
    "          active_logits = logits.view(-1, self.num_labels)\n",
    "\n",
    "        #   active_labels = torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))\n",
    "          loss = loss_fct(active_logits, labels.view(-1))\n",
    "        \n",
    "      return TokenClassifierOutput(logits=logits, loss=loss, hidden_states=outputs.hidden_states,attentions=outputs.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8546, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([64, 42, 26])\n"
     ]
    }
   ],
   "source": [
    "# Check the model output\n",
    "device = \"cpu\"\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "model = HangmanNet(checkpoint=checkpoint, vocab_size = 26, unfreeze_layers = 1).to(device)\n",
    "\n",
    "# Prepare a batch using the custom data collator\n",
    "for batch in train_dataloader:\n",
    "\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "    prev_guess = batch['prev_guess']\n",
    "    prior_probs = batch['prior_probs']\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, \n",
    "                    attention_mask=attention_mask, \n",
    "                    labels=labels, \n",
    "                    prev_guess=prev_guess,\n",
    "                    prior_probs=prior_probs)\n",
    "    print(outputs.loss)\n",
    "    print(outputs.logits.shape)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "model.encoder.layer.10.attention.self.query.weight\n",
      "model.encoder.layer.10.attention.self.query.bias\n",
      "model.encoder.layer.10.attention.self.key.weight\n",
      "model.encoder.layer.10.attention.self.key.bias\n",
      "model.encoder.layer.10.attention.self.value.weight\n",
      "model.encoder.layer.10.attention.self.value.bias\n",
      "model.encoder.layer.10.attention.output.dense.weight\n",
      "model.encoder.layer.10.attention.output.dense.bias\n",
      "model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.10.intermediate.dense.weight\n",
      "model.encoder.layer.10.intermediate.dense.bias\n",
      "model.encoder.layer.10.output.dense.weight\n",
      "model.encoder.layer.10.output.dense.bias\n",
      "model.encoder.layer.10.output.LayerNorm.weight\n",
      "model.encoder.layer.10.output.LayerNorm.bias\n",
      "model.encoder.layer.11.attention.self.query.weight\n",
      "model.encoder.layer.11.attention.self.query.bias\n",
      "model.encoder.layer.11.attention.self.key.weight\n",
      "model.encoder.layer.11.attention.self.key.bias\n",
      "model.encoder.layer.11.attention.self.value.weight\n",
      "model.encoder.layer.11.attention.self.value.bias\n",
      "model.encoder.layer.11.attention.output.dense.weight\n",
      "model.encoder.layer.11.attention.output.dense.bias\n",
      "model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.11.intermediate.dense.weight\n",
      "model.encoder.layer.11.intermediate.dense.bias\n",
      "model.encoder.layer.11.output.dense.weight\n",
      "model.encoder.layer.11.output.dense.bias\n",
      "model.encoder.layer.11.output.LayerNorm.weight\n",
      "model.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.2.weight\n",
      "classifier.2.bias\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "unfreeze_layers = 2\n",
    "model = HangmanNet(checkpoint=checkpoint, vocab_size = 26, unfreeze_layers = unfreeze_layers).to(device)\n",
    "\n",
    "# Print the trainable parameters of the model\n",
    "print(\"Trainable parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "lr = 0.00001\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "num_epochs = 50\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     \"linear\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=num_training_steps,\n",
    "# )\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Prior Information:  torch.Size([28, 26])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "mlm_probability = \"random\" #0.6\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer, \n",
    "                                        #  mlm_probability=mlm_probability, \n",
    "                                         max_length=42, \n",
    "                                         prior_frequencies_path=\"data/total_rel_freq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    data[\"train\"], \n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=True)\n",
    "\n",
    "# Probably add shuffle\n",
    "    \n",
    "eval_dataloader = DataLoader(tokenized_val_data,\n",
    "                             batch_size=64,\n",
    "                             num_workers=8,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. Anadir otra capa al clasifier + anadir al output la prior encoding info\n",
    "# 2. Eliminar el scheduler\n",
    "# 3. Descongelar otra capa\n",
    "# 3. añadir la otra prior information basada en frecuencias\n",
    "# 4. Set la probabilidad aleatoria en lugar de fija\n",
    "# 4. Añadir el early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['model.embeddings.word_embeddings.weight', 'model.embeddings.position_embeddings.weight', 'model.embeddings.token_type_embeddings.weight', 'model.embeddings.LayerNorm.weight', 'model.embeddings.LayerNorm.bias', 'model.encoder.layer.0.attention.self.query.weight', 'model.encoder.layer.0.attention.self.query.bias', 'model.encoder.layer.0.attention.self.key.weight', 'model.encoder.layer.0.attention.self.key.bias', 'model.encoder.layer.0.attention.self.value.weight', 'model.encoder.layer.0.attention.self.value.bias', 'model.encoder.layer.0.attention.output.dense.weight', 'model.encoder.layer.0.attention.output.dense.bias', 'model.encoder.layer.0.attention.output.LayerNorm.weight', 'model.encoder.layer.0.attention.output.LayerNorm.bias', 'model.encoder.layer.0.intermediate.dense.weight', 'model.encoder.layer.0.intermediate.dense.bias', 'model.encoder.layer.0.output.dense.weight', 'model.encoder.layer.0.output.dense.bias', 'model.encoder.layer.0.output.LayerNorm.weight', 'model.encoder.layer.0.output.LayerNorm.bias', 'model.encoder.layer.1.attention.self.query.weight', 'model.encoder.layer.1.attention.self.query.bias', 'model.encoder.layer.1.attention.self.key.weight', 'model.encoder.layer.1.attention.self.key.bias', 'model.encoder.layer.1.attention.self.value.weight', 'model.encoder.layer.1.attention.self.value.bias', 'model.encoder.layer.1.attention.output.dense.weight', 'model.encoder.layer.1.attention.output.dense.bias', 'model.encoder.layer.1.attention.output.LayerNorm.weight', 'model.encoder.layer.1.attention.output.LayerNorm.bias', 'model.encoder.layer.1.intermediate.dense.weight', 'model.encoder.layer.1.intermediate.dense.bias', 'model.encoder.layer.1.output.dense.weight', 'model.encoder.layer.1.output.dense.bias', 'model.encoder.layer.1.output.LayerNorm.weight', 'model.encoder.layer.1.output.LayerNorm.bias', 'model.encoder.layer.2.attention.self.query.weight', 'model.encoder.layer.2.attention.self.query.bias', 'model.encoder.layer.2.attention.self.key.weight', 'model.encoder.layer.2.attention.self.key.bias', 'model.encoder.layer.2.attention.self.value.weight', 'model.encoder.layer.2.attention.self.value.bias', 'model.encoder.layer.2.attention.output.dense.weight', 'model.encoder.layer.2.attention.output.dense.bias', 'model.encoder.layer.2.attention.output.LayerNorm.weight', 'model.encoder.layer.2.attention.output.LayerNorm.bias', 'model.encoder.layer.2.intermediate.dense.weight', 'model.encoder.layer.2.intermediate.dense.bias', 'model.encoder.layer.2.output.dense.weight', 'model.encoder.layer.2.output.dense.bias', 'model.encoder.layer.2.output.LayerNorm.weight', 'model.encoder.layer.2.output.LayerNorm.bias', 'model.encoder.layer.3.attention.self.query.weight', 'model.encoder.layer.3.attention.self.query.bias', 'model.encoder.layer.3.attention.self.key.weight', 'model.encoder.layer.3.attention.self.key.bias', 'model.encoder.layer.3.attention.self.value.weight', 'model.encoder.layer.3.attention.self.value.bias', 'model.encoder.layer.3.attention.output.dense.weight', 'model.encoder.layer.3.attention.output.dense.bias', 'model.encoder.layer.3.attention.output.LayerNorm.weight', 'model.encoder.layer.3.attention.output.LayerNorm.bias', 'model.encoder.layer.3.intermediate.dense.weight', 'model.encoder.layer.3.intermediate.dense.bias', 'model.encoder.layer.3.output.dense.weight', 'model.encoder.layer.3.output.dense.bias', 'model.encoder.layer.3.output.LayerNorm.weight', 'model.encoder.layer.3.output.LayerNorm.bias', 'model.encoder.layer.4.attention.self.query.weight', 'model.encoder.layer.4.attention.self.query.bias', 'model.encoder.layer.4.attention.self.key.weight', 'model.encoder.layer.4.attention.self.key.bias', 'model.encoder.layer.4.attention.self.value.weight', 'model.encoder.layer.4.attention.self.value.bias', 'model.encoder.layer.4.attention.output.dense.weight', 'model.encoder.layer.4.attention.output.dense.bias', 'model.encoder.layer.4.attention.output.LayerNorm.weight', 'model.encoder.layer.4.attention.output.LayerNorm.bias', 'model.encoder.layer.4.intermediate.dense.weight', 'model.encoder.layer.4.intermediate.dense.bias', 'model.encoder.layer.4.output.dense.weight', 'model.encoder.layer.4.output.dense.bias', 'model.encoder.layer.4.output.LayerNorm.weight', 'model.encoder.layer.4.output.LayerNorm.bias', 'model.encoder.layer.5.attention.self.query.weight', 'model.encoder.layer.5.attention.self.query.bias', 'model.encoder.layer.5.attention.self.key.weight', 'model.encoder.layer.5.attention.self.key.bias', 'model.encoder.layer.5.attention.self.value.weight', 'model.encoder.layer.5.attention.self.value.bias', 'model.encoder.layer.5.attention.output.dense.weight', 'model.encoder.layer.5.attention.output.dense.bias', 'model.encoder.layer.5.attention.output.LayerNorm.weight', 'model.encoder.layer.5.attention.output.LayerNorm.bias', 'model.encoder.layer.5.intermediate.dense.weight', 'model.encoder.layer.5.intermediate.dense.bias', 'model.encoder.layer.5.output.dense.weight', 'model.encoder.layer.5.output.dense.bias', 'model.encoder.layer.5.output.LayerNorm.weight', 'model.encoder.layer.5.output.LayerNorm.bias', 'model.encoder.layer.6.attention.self.query.weight', 'model.encoder.layer.6.attention.self.query.bias', 'model.encoder.layer.6.attention.self.key.weight', 'model.encoder.layer.6.attention.self.key.bias', 'model.encoder.layer.6.attention.self.value.weight', 'model.encoder.layer.6.attention.self.value.bias', 'model.encoder.layer.6.attention.output.dense.weight', 'model.encoder.layer.6.attention.output.dense.bias', 'model.encoder.layer.6.attention.output.LayerNorm.weight', 'model.encoder.layer.6.attention.output.LayerNorm.bias', 'model.encoder.layer.6.intermediate.dense.weight', 'model.encoder.layer.6.intermediate.dense.bias', 'model.encoder.layer.6.output.dense.weight', 'model.encoder.layer.6.output.dense.bias', 'model.encoder.layer.6.output.LayerNorm.weight', 'model.encoder.layer.6.output.LayerNorm.bias', 'model.encoder.layer.7.attention.self.query.weight', 'model.encoder.layer.7.attention.self.query.bias', 'model.encoder.layer.7.attention.self.key.weight', 'model.encoder.layer.7.attention.self.key.bias', 'model.encoder.layer.7.attention.self.value.weight', 'model.encoder.layer.7.attention.self.value.bias', 'model.encoder.layer.7.attention.output.dense.weight', 'model.encoder.layer.7.attention.output.dense.bias', 'model.encoder.layer.7.attention.output.LayerNorm.weight', 'model.encoder.layer.7.attention.output.LayerNorm.bias', 'model.encoder.layer.7.intermediate.dense.weight', 'model.encoder.layer.7.intermediate.dense.bias', 'model.encoder.layer.7.output.dense.weight', 'model.encoder.layer.7.output.dense.bias', 'model.encoder.layer.7.output.LayerNorm.weight', 'model.encoder.layer.7.output.LayerNorm.bias', 'model.encoder.layer.8.attention.self.query.weight', 'model.encoder.layer.8.attention.self.query.bias', 'model.encoder.layer.8.attention.self.key.weight', 'model.encoder.layer.8.attention.self.key.bias', 'model.encoder.layer.8.attention.self.value.weight', 'model.encoder.layer.8.attention.self.value.bias', 'model.encoder.layer.8.attention.output.dense.weight', 'model.encoder.layer.8.attention.output.dense.bias', 'model.encoder.layer.8.attention.output.LayerNorm.weight', 'model.encoder.layer.8.attention.output.LayerNorm.bias', 'model.encoder.layer.8.intermediate.dense.weight', 'model.encoder.layer.8.intermediate.dense.bias', 'model.encoder.layer.8.output.dense.weight', 'model.encoder.layer.8.output.dense.bias', 'model.encoder.layer.8.output.LayerNorm.weight', 'model.encoder.layer.8.output.LayerNorm.bias', 'model.encoder.layer.9.attention.self.query.weight', 'model.encoder.layer.9.attention.self.query.bias', 'model.encoder.layer.9.attention.self.key.weight', 'model.encoder.layer.9.attention.self.key.bias', 'model.encoder.layer.9.attention.self.value.weight', 'model.encoder.layer.9.attention.self.value.bias', 'model.encoder.layer.9.attention.output.dense.weight', 'model.encoder.layer.9.attention.output.dense.bias', 'model.encoder.layer.9.attention.output.LayerNorm.weight', 'model.encoder.layer.9.attention.output.LayerNorm.bias', 'model.encoder.layer.9.intermediate.dense.weight', 'model.encoder.layer.9.intermediate.dense.bias', 'model.encoder.layer.9.output.dense.weight', 'model.encoder.layer.9.output.dense.bias', 'model.encoder.layer.9.output.LayerNorm.weight', 'model.encoder.layer.9.output.LayerNorm.bias', 'model.encoder.layer.10.attention.self.query.weight', 'model.encoder.layer.10.attention.self.query.bias', 'model.encoder.layer.10.attention.self.key.weight', 'model.encoder.layer.10.attention.self.key.bias', 'model.encoder.layer.10.attention.self.value.weight', 'model.encoder.layer.10.attention.self.value.bias', 'model.encoder.layer.10.attention.output.dense.weight', 'model.encoder.layer.10.attention.output.dense.bias', 'model.encoder.layer.10.attention.output.LayerNorm.weight', 'model.encoder.layer.10.attention.output.LayerNorm.bias', 'model.encoder.layer.10.intermediate.dense.weight', 'model.encoder.layer.10.intermediate.dense.bias', 'model.encoder.layer.10.output.dense.weight', 'model.encoder.layer.10.output.dense.bias', 'model.encoder.layer.10.output.LayerNorm.weight', 'model.encoder.layer.10.output.LayerNorm.bias', 'model.encoder.layer.11.attention.self.query.weight', 'model.encoder.layer.11.attention.self.query.bias', 'model.encoder.layer.11.attention.self.key.weight', 'model.encoder.layer.11.attention.self.key.bias', 'model.encoder.layer.11.attention.self.value.weight', 'model.encoder.layer.11.attention.self.value.bias', 'model.encoder.layer.11.attention.output.dense.weight', 'model.encoder.layer.11.attention.output.dense.bias', 'model.encoder.layer.11.attention.output.LayerNorm.weight', 'model.encoder.layer.11.attention.output.LayerNorm.bias', 'model.encoder.layer.11.intermediate.dense.weight', 'model.encoder.layer.11.intermediate.dense.bias', 'model.encoder.layer.11.output.dense.weight', 'model.encoder.layer.11.output.dense.bias', 'model.encoder.layer.11.output.LayerNorm.weight', 'model.encoder.layer.11.output.LayerNorm.bias', 'model.pooler.dense.weight', 'model.pooler.dense.bias', 'classifier.0.weight', 'classifier.0.bias', 'classifier.2.weight', 'classifier.2.bias'])\n",
      "dict_keys(['model.embeddings.word_embeddings.weight', 'model.embeddings.position_embeddings.weight', 'model.embeddings.token_type_embeddings.weight', 'model.embeddings.LayerNorm.weight', 'model.embeddings.LayerNorm.bias', 'model.encoder.layer.0.attention.self.query.weight', 'model.encoder.layer.0.attention.self.query.bias', 'model.encoder.layer.0.attention.self.key.weight', 'model.encoder.layer.0.attention.self.key.bias', 'model.encoder.layer.0.attention.self.value.weight', 'model.encoder.layer.0.attention.self.value.bias', 'model.encoder.layer.0.attention.output.dense.weight', 'model.encoder.layer.0.attention.output.dense.bias', 'model.encoder.layer.0.attention.output.LayerNorm.weight', 'model.encoder.layer.0.attention.output.LayerNorm.bias', 'model.encoder.layer.0.intermediate.dense.weight', 'model.encoder.layer.0.intermediate.dense.bias', 'model.encoder.layer.0.output.dense.weight', 'model.encoder.layer.0.output.dense.bias', 'model.encoder.layer.0.output.LayerNorm.weight', 'model.encoder.layer.0.output.LayerNorm.bias', 'model.encoder.layer.1.attention.self.query.weight', 'model.encoder.layer.1.attention.self.query.bias', 'model.encoder.layer.1.attention.self.key.weight', 'model.encoder.layer.1.attention.self.key.bias', 'model.encoder.layer.1.attention.self.value.weight', 'model.encoder.layer.1.attention.self.value.bias', 'model.encoder.layer.1.attention.output.dense.weight', 'model.encoder.layer.1.attention.output.dense.bias', 'model.encoder.layer.1.attention.output.LayerNorm.weight', 'model.encoder.layer.1.attention.output.LayerNorm.bias', 'model.encoder.layer.1.intermediate.dense.weight', 'model.encoder.layer.1.intermediate.dense.bias', 'model.encoder.layer.1.output.dense.weight', 'model.encoder.layer.1.output.dense.bias', 'model.encoder.layer.1.output.LayerNorm.weight', 'model.encoder.layer.1.output.LayerNorm.bias', 'model.encoder.layer.2.attention.self.query.weight', 'model.encoder.layer.2.attention.self.query.bias', 'model.encoder.layer.2.attention.self.key.weight', 'model.encoder.layer.2.attention.self.key.bias', 'model.encoder.layer.2.attention.self.value.weight', 'model.encoder.layer.2.attention.self.value.bias', 'model.encoder.layer.2.attention.output.dense.weight', 'model.encoder.layer.2.attention.output.dense.bias', 'model.encoder.layer.2.attention.output.LayerNorm.weight', 'model.encoder.layer.2.attention.output.LayerNorm.bias', 'model.encoder.layer.2.intermediate.dense.weight', 'model.encoder.layer.2.intermediate.dense.bias', 'model.encoder.layer.2.output.dense.weight', 'model.encoder.layer.2.output.dense.bias', 'model.encoder.layer.2.output.LayerNorm.weight', 'model.encoder.layer.2.output.LayerNorm.bias', 'model.encoder.layer.3.attention.self.query.weight', 'model.encoder.layer.3.attention.self.query.bias', 'model.encoder.layer.3.attention.self.key.weight', 'model.encoder.layer.3.attention.self.key.bias', 'model.encoder.layer.3.attention.self.value.weight', 'model.encoder.layer.3.attention.self.value.bias', 'model.encoder.layer.3.attention.output.dense.weight', 'model.encoder.layer.3.attention.output.dense.bias', 'model.encoder.layer.3.attention.output.LayerNorm.weight', 'model.encoder.layer.3.attention.output.LayerNorm.bias', 'model.encoder.layer.3.intermediate.dense.weight', 'model.encoder.layer.3.intermediate.dense.bias', 'model.encoder.layer.3.output.dense.weight', 'model.encoder.layer.3.output.dense.bias', 'model.encoder.layer.3.output.LayerNorm.weight', 'model.encoder.layer.3.output.LayerNorm.bias', 'model.encoder.layer.4.attention.self.query.weight', 'model.encoder.layer.4.attention.self.query.bias', 'model.encoder.layer.4.attention.self.key.weight', 'model.encoder.layer.4.attention.self.key.bias', 'model.encoder.layer.4.attention.self.value.weight', 'model.encoder.layer.4.attention.self.value.bias', 'model.encoder.layer.4.attention.output.dense.weight', 'model.encoder.layer.4.attention.output.dense.bias', 'model.encoder.layer.4.attention.output.LayerNorm.weight', 'model.encoder.layer.4.attention.output.LayerNorm.bias', 'model.encoder.layer.4.intermediate.dense.weight', 'model.encoder.layer.4.intermediate.dense.bias', 'model.encoder.layer.4.output.dense.weight', 'model.encoder.layer.4.output.dense.bias', 'model.encoder.layer.4.output.LayerNorm.weight', 'model.encoder.layer.4.output.LayerNorm.bias', 'model.encoder.layer.5.attention.self.query.weight', 'model.encoder.layer.5.attention.self.query.bias', 'model.encoder.layer.5.attention.self.key.weight', 'model.encoder.layer.5.attention.self.key.bias', 'model.encoder.layer.5.attention.self.value.weight', 'model.encoder.layer.5.attention.self.value.bias', 'model.encoder.layer.5.attention.output.dense.weight', 'model.encoder.layer.5.attention.output.dense.bias', 'model.encoder.layer.5.attention.output.LayerNorm.weight', 'model.encoder.layer.5.attention.output.LayerNorm.bias', 'model.encoder.layer.5.intermediate.dense.weight', 'model.encoder.layer.5.intermediate.dense.bias', 'model.encoder.layer.5.output.dense.weight', 'model.encoder.layer.5.output.dense.bias', 'model.encoder.layer.5.output.LayerNorm.weight', 'model.encoder.layer.5.output.LayerNorm.bias', 'model.encoder.layer.6.attention.self.query.weight', 'model.encoder.layer.6.attention.self.query.bias', 'model.encoder.layer.6.attention.self.key.weight', 'model.encoder.layer.6.attention.self.key.bias', 'model.encoder.layer.6.attention.self.value.weight', 'model.encoder.layer.6.attention.self.value.bias', 'model.encoder.layer.6.attention.output.dense.weight', 'model.encoder.layer.6.attention.output.dense.bias', 'model.encoder.layer.6.attention.output.LayerNorm.weight', 'model.encoder.layer.6.attention.output.LayerNorm.bias', 'model.encoder.layer.6.intermediate.dense.weight', 'model.encoder.layer.6.intermediate.dense.bias', 'model.encoder.layer.6.output.dense.weight', 'model.encoder.layer.6.output.dense.bias', 'model.encoder.layer.6.output.LayerNorm.weight', 'model.encoder.layer.6.output.LayerNorm.bias', 'model.encoder.layer.7.attention.self.query.weight', 'model.encoder.layer.7.attention.self.query.bias', 'model.encoder.layer.7.attention.self.key.weight', 'model.encoder.layer.7.attention.self.key.bias', 'model.encoder.layer.7.attention.self.value.weight', 'model.encoder.layer.7.attention.self.value.bias', 'model.encoder.layer.7.attention.output.dense.weight', 'model.encoder.layer.7.attention.output.dense.bias', 'model.encoder.layer.7.attention.output.LayerNorm.weight', 'model.encoder.layer.7.attention.output.LayerNorm.bias', 'model.encoder.layer.7.intermediate.dense.weight', 'model.encoder.layer.7.intermediate.dense.bias', 'model.encoder.layer.7.output.dense.weight', 'model.encoder.layer.7.output.dense.bias', 'model.encoder.layer.7.output.LayerNorm.weight', 'model.encoder.layer.7.output.LayerNorm.bias', 'model.encoder.layer.8.attention.self.query.weight', 'model.encoder.layer.8.attention.self.query.bias', 'model.encoder.layer.8.attention.self.key.weight', 'model.encoder.layer.8.attention.self.key.bias', 'model.encoder.layer.8.attention.self.value.weight', 'model.encoder.layer.8.attention.self.value.bias', 'model.encoder.layer.8.attention.output.dense.weight', 'model.encoder.layer.8.attention.output.dense.bias', 'model.encoder.layer.8.attention.output.LayerNorm.weight', 'model.encoder.layer.8.attention.output.LayerNorm.bias', 'model.encoder.layer.8.intermediate.dense.weight', 'model.encoder.layer.8.intermediate.dense.bias', 'model.encoder.layer.8.output.dense.weight', 'model.encoder.layer.8.output.dense.bias', 'model.encoder.layer.8.output.LayerNorm.weight', 'model.encoder.layer.8.output.LayerNorm.bias', 'model.encoder.layer.9.attention.self.query.weight', 'model.encoder.layer.9.attention.self.query.bias', 'model.encoder.layer.9.attention.self.key.weight', 'model.encoder.layer.9.attention.self.key.bias', 'model.encoder.layer.9.attention.self.value.weight', 'model.encoder.layer.9.attention.self.value.bias', 'model.encoder.layer.9.attention.output.dense.weight', 'model.encoder.layer.9.attention.output.dense.bias', 'model.encoder.layer.9.attention.output.LayerNorm.weight', 'model.encoder.layer.9.attention.output.LayerNorm.bias', 'model.encoder.layer.9.intermediate.dense.weight', 'model.encoder.layer.9.intermediate.dense.bias', 'model.encoder.layer.9.output.dense.weight', 'model.encoder.layer.9.output.dense.bias', 'model.encoder.layer.9.output.LayerNorm.weight', 'model.encoder.layer.9.output.LayerNorm.bias', 'model.encoder.layer.10.attention.self.query.weight', 'model.encoder.layer.10.attention.self.query.bias', 'model.encoder.layer.10.attention.self.key.weight', 'model.encoder.layer.10.attention.self.key.bias', 'model.encoder.layer.10.attention.self.value.weight', 'model.encoder.layer.10.attention.self.value.bias', 'model.encoder.layer.10.attention.output.dense.weight', 'model.encoder.layer.10.attention.output.dense.bias', 'model.encoder.layer.10.attention.output.LayerNorm.weight', 'model.encoder.layer.10.attention.output.LayerNorm.bias', 'model.encoder.layer.10.intermediate.dense.weight', 'model.encoder.layer.10.intermediate.dense.bias', 'model.encoder.layer.10.output.dense.weight', 'model.encoder.layer.10.output.dense.bias', 'model.encoder.layer.10.output.LayerNorm.weight', 'model.encoder.layer.10.output.LayerNorm.bias', 'model.encoder.layer.11.attention.self.query.weight', 'model.encoder.layer.11.attention.self.query.bias', 'model.encoder.layer.11.attention.self.key.weight', 'model.encoder.layer.11.attention.self.key.bias', 'model.encoder.layer.11.attention.self.value.weight', 'model.encoder.layer.11.attention.self.value.bias', 'model.encoder.layer.11.attention.output.dense.weight', 'model.encoder.layer.11.attention.output.dense.bias', 'model.encoder.layer.11.attention.output.LayerNorm.weight', 'model.encoder.layer.11.attention.output.LayerNorm.bias', 'model.encoder.layer.11.intermediate.dense.weight', 'model.encoder.layer.11.intermediate.dense.bias', 'model.encoder.layer.11.output.dense.weight', 'model.encoder.layer.11.output.dense.bias', 'model.encoder.layer.11.output.LayerNorm.weight', 'model.encoder.layer.11.output.LayerNorm.bias', 'model.pooler.dense.weight', 'model.pooler.dense.bias'])\n",
      "dict_keys(['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model checkpoint\n",
    "checkpoint_path = \"models/basic_models/model_lr_5e-05_mlm_prob_0.5/model_epoch_50.pth\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Drop the last layer of the classifier\n",
    "model_checkpoint = checkpoint['model_state_dict']\n",
    "print(model_checkpoint.keys())\n",
    "\n",
    "model_checkpoint = {k: v for k, v in model_checkpoint.items() if 'classifier' not in k}\n",
    "print(model_checkpoint.keys())\n",
    "\n",
    "# Remove the word mode from model_checkpoint keys\n",
    "model_checkpoint = {k.replace('model.',''): v for k, v in model_checkpoint.items()}\n",
    "print(model_checkpoint.keys())\n",
    "\n",
    "model.model.load_state_dict(model_checkpoint)\n",
    "\n",
    "# Print the trainable parameters of the model\n",
    "# print(\"Trainable parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50: 100%|██████████| 3197/3197 [05:09<00:00, 10.32it/s]\n",
      "Validation Epoch 1/50: 100%|██████████| 356/356 [00:26<00:00, 13.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Training Loss: 1.5807\n",
      "Validation Loss: 1.7977\n",
      "Validation Accuracy: 0.4063\n",
      "Validation F1 Score: 0.3005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/50: 100%|██████████| 3197/3197 [05:27<00:00,  9.77it/s]\n",
      "Validation Epoch 2/50: 100%|██████████| 356/356 [00:26<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "Training Loss: 1.3796\n",
      "Validation Loss: 1.7616\n",
      "Validation Accuracy: 0.4138\n",
      "Validation F1 Score: 0.3392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/50: 100%|██████████| 3197/3197 [05:28<00:00,  9.74it/s]\n",
      "Validation Epoch 3/50: 100%|██████████| 356/356 [00:27<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "Training Loss: 1.3573\n",
      "Validation Loss: 1.7503\n",
      "Validation Accuracy: 0.4156\n",
      "Validation F1 Score: 0.3456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/50: 100%|██████████| 3197/3197 [05:28<00:00,  9.72it/s]\n",
      "Validation Epoch 4/50: 100%|██████████| 356/356 [00:27<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "Training Loss: 1.3500\n",
      "Validation Loss: 1.7439\n",
      "Validation Accuracy: 0.4176\n",
      "Validation F1 Score: 0.3492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/50: 100%|██████████| 3197/3197 [05:28<00:00,  9.74it/s]\n",
      "Validation Epoch 5/50: 100%|██████████| 356/356 [00:27<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "Training Loss: 1.3445\n",
      "Validation Loss: 1.7401\n",
      "Validation Accuracy: 0.4177\n",
      "Validation F1 Score: 0.3504\n",
      "Model saved to models/prior_prob_models/model_lr_1e-05_mlm_prob_random/model_epoch_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/50: 100%|██████████| 3197/3197 [05:28<00:00,  9.72it/s]\n",
      "Validation Epoch 6/50: 100%|██████████| 356/356 [00:27<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "Training Loss: 1.3395\n",
      "Validation Loss: 1.7376\n",
      "Validation Accuracy: 0.4177\n",
      "Validation F1 Score: 0.3503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/50: 100%|██████████| 3197/3197 [05:28<00:00,  9.72it/s]\n",
      "Validation Epoch 7/50: 100%|██████████| 356/356 [00:27<00:00, 13.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "Training Loss: 1.3372\n",
      "Validation Loss: 1.7354\n",
      "Validation Accuracy: 0.4185\n",
      "Validation F1 Score: 0.3527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/50: 100%|██████████| 3197/3197 [05:29<00:00,  9.70it/s]\n",
      "Validation Epoch 8/50: 100%|██████████| 356/356 [00:27<00:00, 13.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "Training Loss: 1.3352\n",
      "Validation Loss: 1.7349\n",
      "Validation Accuracy: 0.4183\n",
      "Validation F1 Score: 0.3540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/50: 100%|██████████| 3197/3197 [05:32<00:00,  9.62it/s]\n",
      "Validation Epoch 9/50: 100%|██████████| 356/356 [00:27<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "Training Loss: 1.3321\n",
      "Validation Loss: 1.7322\n",
      "Validation Accuracy: 0.4186\n",
      "Validation F1 Score: 0.3542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/50: 100%|██████████| 3197/3197 [05:33<00:00,  9.58it/s]\n",
      "Validation Epoch 10/50: 100%|██████████| 356/356 [00:27<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "Training Loss: 1.3333\n",
      "Validation Loss: 1.7315\n",
      "Validation Accuracy: 0.4188\n",
      "Validation F1 Score: 0.3568\n",
      "Model saved to models/prior_prob_models/model_lr_1e-05_mlm_prob_random/model_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/50:  98%|█████████▊| 3125/3197 [05:25<00:07,  9.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# lr_scheduler.step()\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 52\u001b[0m     total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m     55\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "save_dir = \"models/prior_prob_models\"\n",
    "save_dir = os.path.join(save_dir, f\"model_lr_{lr}_mlm_prob_{mlm_probability}\")\n",
    "writer = SummaryWriter(log_dir=save_dir)\n",
    "\n",
    "# Initialize metrics storage\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_f1_scores = []\n",
    "\n",
    "# Define the save frequency\n",
    "save_frequency = 5  # Save model every n epochs, adjust this as needed\n",
    "\n",
    "# Load the model checkpoint if available\n",
    "checkpoint_path = \"path_to_checkpoint\"  # Provide the path to your saved checkpoint\n",
    "start_epoch = 0\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    val_accuracies = checkpoint['val_accuracies']\n",
    "    val_f1_scores = checkpoint['val_f1_scores']\n",
    "    print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        # lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        flat_predictions = predictions.view(-1)\n",
    "        flat_references = batch[\"labels\"].view(-1)\n",
    "        flat_attention_mask = batch[\"labels\"].view(-1) != -100\n",
    "\n",
    "        active_predictions = flat_predictions[flat_attention_mask]\n",
    "        active_references = flat_references[flat_attention_mask]\n",
    "\n",
    "        all_predictions.extend(active_predictions.cpu().numpy())\n",
    "        all_references.extend(active_references.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(eval_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    val_accuracy = accuracy_score(all_references, all_predictions)\n",
    "    val_f1 = f1_score(all_references, all_predictions, average=\"macro\")\n",
    "\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalars('Loss', {'train_loss': avg_train_loss, 'val_loss' : avg_val_loss}, epoch)\n",
    "    writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "    # Save the model every n epochs\n",
    "    if (epoch + 1) % save_frequency == 0:\n",
    "        model_save_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pth\")\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'val_f1_scores': val_f1_scores\n",
    "        }\n",
    "        torch.save(checkpoint, model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save the final model\n",
    "model_save_path = os.path.join(save_dir, \"model_final.pth\")\n",
    "checkpoint = {\n",
    "    'epoch': epoch + 1,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'val_f1_scores': val_f1_scores\n",
    "}\n",
    "torch.save(checkpoint, model_save_path)\n",
    "print(f\"Final model saved to {model_save_path}\")\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning and CharacterPredictor Head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://stackoverflow.com/questions/69907682/what-are-differences-between-automodelforsequenceclassification-vs-automodel\n",
    "\n",
    "https://huggingface.co/learn/nlp-course/chapter3/4?fw=pt\n",
    "\n",
    "https://aclanthology.org/2020.coling-main.609.pdf\n",
    "\n",
    "https://github.com/helboukkouri/character-bert?tab=readme-ov-file#how-do-i-pre-train-characterbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abandoned\n",
      "tensor([[57344,    97,    98,    97,   110,   100,   111,   110,   101,   100,\n",
      "         57345]])\n",
      "torch.Size([1, 11, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Searching for a character-level model\n",
    "model_name = \"google/canine-s\"  # Example: Canine model from Google Research\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Example input (a word split into characters)\n",
    "input_text = \"a b a n d o n e d\".replace(\" \", \"\")\n",
    "\n",
    "print(input_text)\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "\n",
    "print(inputs['input_ids'])\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CanineModel(\n",
       "  (char_embeddings): CanineEmbeddings(\n",
       "    (HashBucketCodepointEmbedder_0): Embedding(16384, 96)\n",
       "    (HashBucketCodepointEmbedder_1): Embedding(16384, 96)\n",
       "    (HashBucketCodepointEmbedder_2): Embedding(16384, 96)\n",
       "    (HashBucketCodepointEmbedder_3): Embedding(16384, 96)\n",
       "    (HashBucketCodepointEmbedder_4): Embedding(16384, 96)\n",
       "    (HashBucketCodepointEmbedder_5): Embedding(16384, 96)\n",
       "    (HashBucketCodepointEmbedder_6): Embedding(16384, 96)\n",
       "    (HashBucketCodepointEmbedder_7): Embedding(16384, 96)\n",
       "    (char_position_embeddings): Embedding(16384, 768)\n",
       "    (token_type_embeddings): Embedding(16, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (initial_char_encoder): CanineEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): CanineLayer(\n",
       "        (attention): CanineAttention(\n",
       "          (self): CanineSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): CanineSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): CanineIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): CanineOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (chars_to_molecules): CharactersToMolecules(\n",
       "    (conv): Conv1d(768, 768, kernel_size=(4,), stride=(4,))\n",
       "    (activation): GELUActivation()\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (encoder): CanineEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x CanineLayer(\n",
       "        (attention): CanineAttention(\n",
       "          (self): CanineSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): CanineSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): CanineIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): CanineOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projection): ConvProjection(\n",
       "    (conv): Conv1d(1536, 768, kernel_size=(4,), stride=(1,))\n",
       "    (activation): GELUActivation()\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (final_char_encoder): CanineEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): CanineLayer(\n",
       "        (attention): CanineAttention(\n",
       "          (self): CanineSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): CanineSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): CanineIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): CanineOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): CaninePooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "character-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
