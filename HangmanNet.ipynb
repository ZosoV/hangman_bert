{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs\n",
    "# _ _ _ _ _\n",
    "\n",
    "# 250000 words\n",
    "\n",
    "# El potter no [MASK] novia\n",
    "\n",
    "# p _ t t e r\n",
    "\n",
    "\n",
    "# 1. Filtrar las palabras\n",
    "# 2. Palabras mas comunes\n",
    "# 3. Data colador\n",
    "# 4. Penalize more small words??\n",
    "# 5. Podria concatenar las palabras adivinadas hasta ahora al final del classifier, es practicamente concatenar un one-hot vector \n",
    "# de 0-26 con 1 donde las palabras ya fueron adivinadas.\n",
    "# 6. Podria usar one-hot encodings 0-27 para los characteres\n",
    "# 7. El Test y el val tambien generalo de manera dinamica. No te preocupes por que distribucion tendra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict, Dataset\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Set a seed for all libraries\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word  word_length  unique_chars\n",
      "0      timpani            7             6\n",
      "1       worsle            6             6\n",
      "2        yinst            5             5\n",
      "3  grangerized           11             8\n",
      "4      matatua            7             4\n",
      "(204570, 3)\n",
      "(22730, 3)\n",
      "(130436, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets \n",
    "df_train = pd.read_csv('data/to_mask_train.csv', converters={'word': str})\n",
    "df_val = pd.read_csv('data/masked_val.csv', converters={'masked_word': str, 'labels': str, 'previous_guesses': eval})\n",
    "df_test = pd.read_csv('data/masked_test.csv', converters={'masked_word': str, 'labels': str, 'previous_guesses': eval})\n",
    "\n",
    "print(df_train.head())\n",
    "\n",
    "# Add spaces to the word in training\n",
    "df_train[\"word\"] = df_train[\"word\"].apply(lambda x: ' '.join(list(x)))\n",
    "\n",
    "# Add spaces to the masked_word and labels\n",
    "df_val[\"masked_word\"] = df_val[\"masked_word\"].apply(lambda x: ' '.join(list(x)))\n",
    "df_val[\"labels\"] = df_val[\"labels\"].apply(lambda x: ' '.join(list(x)))\n",
    "df_test[\"masked_word\"] = df_test[\"masked_word\"].apply(lambda x: ' '.join(list(x)))\n",
    "df_test[\"labels\"] = df_test[\"labels\"].apply(lambda x: ' '.join(list(x)))\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>word_length</th>\n",
       "      <th>unique_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t i m p a n i</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>w o r s l e</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>y i n s t</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g r a n g e r i z e d</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m a t a t u a</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    word  word_length  unique_chars\n",
       "0          t i m p a n i            7             6\n",
       "1            w o r s l e            6             6\n",
       "2              y i n s t            5             5\n",
       "3  g r a n g e r i z e d           11             8\n",
       "4          m a t a t u a            7             4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_word</th>\n",
       "      <th>labels</th>\n",
       "      <th>previous_guesses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h e _ _ c l _ t _ s</td>\n",
       "      <td>h e r a c l i t u s</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_ e _ o d _ _ _</td>\n",
       "      <td>m e r o d a c h</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_ _ g _ _ _</td>\n",
       "      <td>i n g i r t</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_ _ _ z u o k a</td>\n",
       "      <td>s h i z u o k a</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m _ _ t _ _ _ _ n n e _ _ e d</td>\n",
       "      <td>m u l t i c h a n n e l l e d</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     masked_word                         labels  \\\n",
       "0            h e _ _ c l _ t _ s            h e r a c l i t u s   \n",
       "1                _ e _ o d _ _ _                m e r o d a c h   \n",
       "2                    _ _ g _ _ _                    i n g i r t   \n",
       "3                _ _ _ z u o k a                s h i z u o k a   \n",
       "4  m _ _ t _ _ _ _ n n e _ _ e d  m u l t i c h a n n e l l e d   \n",
       "\n",
       "                                    previous_guesses  \n",
       "0  [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...  \n",
       "4  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['word'],\n",
       "        num_rows: 204570\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['masked_word', 'labels', 'previous_guesses'],\n",
       "        num_rows: 22730\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['masked_word', 'labels', 'previous_guesses'],\n",
       "        num_rows: 130436\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather all the data in a DatasetDict\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train[[\"word\"]]),\n",
    "    \"valid\": Dataset.from_pandas(df_val),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling, BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataCollatorForMLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer, mlm_probability=None, max_length=42):\n",
    "        # Take a random value from 0.3 to 0.8 if mlm_probability is None\n",
    "        if mlm_probability is None:\n",
    "            mlm_probability = random.uniform(0.3, 0.8)\n",
    "\n",
    "        super().__init__(tokenizer=tokenizer, mlm_probability=mlm_probability)\n",
    "        self.max_length = max_length\n",
    "        self.char_to_index = {chr(i + ord('a')): i for i in range(26)}\n",
    "\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "\n",
    "        # Convert characters to their corresponding numerical labels\n",
    "        labels = []\n",
    "        words = []\n",
    "        for example in examples:\n",
    "            example = example['word']\n",
    "            row_label = [self.char_to_index[char] for char in example.split()]\n",
    "            labels.append(row_label)\n",
    "            words.append(example)\n",
    "\n",
    "        # Pad labels to the same length (considering special tokens at the beginning and end of each label)\n",
    "        labels = [ [-100] + row_label + [-100] * (self.max_length - len(row_label) - 1) for row_label in labels]\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # Create a one-hot vector for the labels without consider the -100 values\n",
    "        prev_guess = torch.zeros((labels.shape[0], 26), dtype=torch.int64)\n",
    "        for i, label in enumerate(labels):\n",
    "            prev_guess[i][label[label != -100]] = 1\n",
    "\n",
    "            # NOTE: I could add until 5 random previous guesses to the prev_guess tensor\n",
    "            #  to account for maximum number of mistakes made when playing Hangman\n",
    "            random_guesses = torch.tensor([i for i in range(26) if i not in label])\n",
    "\n",
    "            # Take from 0 to 5 values from the random guesses randomly\n",
    "            if len(random_guesses) > 5:\n",
    "                random_guesses = random_guesses[torch.randperm(len(random_guesses))[:5]]\n",
    "                prev_guess[i][random_guesses] = 1\n",
    "\n",
    "        # Tokenize and pad the input examples\n",
    "        batch = self.tokenizer(words, truncation=True, padding='max_length', return_tensors=\"pt\", max_length=self.max_length)\n",
    "        \n",
    "        # Get the input_ids and apply masking\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        # labels = input_ids.clone()\n",
    "\n",
    "        for i, input_id in enumerate(input_ids):\n",
    "\n",
    "            # Get unique tokens\n",
    "            unique_tokens = torch.unique(input_id)\n",
    "            \n",
    "            # Filter Special Tokens by setting probability to 0.0\n",
    "            special_tokens_mask = self.tokenizer.get_special_tokens_mask(unique_tokens, already_has_special_tokens=True)\n",
    "            probabilities = torch.full(unique_tokens.shape, self.mlm_probability)\n",
    "            probabilities.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "            \n",
    "            # Filter the tokens to mask\n",
    "            tokens_to_mask = unique_tokens[torch.bernoulli(probabilities).bool()]\n",
    "\n",
    "            # Mask all instances of the chosen tokens\n",
    "            masked_indices = torch.zeros(input_id.shape, dtype=torch.bool)\n",
    "            for token in tokens_to_mask:\n",
    "                masked_indices[(input_id == token)] = True\n",
    "\n",
    "            labels[i][~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "            # 80% of the time, replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "            indices_replaced = torch.bernoulli(torch.full(input_id.shape, 0.8)).bool() & masked_indices\n",
    "            input_ids[i][indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "            # 10% of the time, replace masked input tokens with random word\n",
    "            # indices_random = torch.bernoulli(torch.full(input_id.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "            # random_words = torch.randint(26, input_id.shape, dtype=torch.long)\n",
    "            # input_ids[i][indices_random] = random_words[indices_random]\n",
    "\n",
    "            # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "\n",
    "        # Set to zero the values masked in the labels\n",
    "        for i, label in enumerate(labels):\n",
    "            prev_guess[i][label[label != -100]] = 0\n",
    "\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"prev_guess\"] = prev_guess\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word\n",
      "0    h e l l o\n",
      "1    w o r l d\n",
      "2    p a r i s\n",
      "3  b e r l i n\n",
      "Dataset({\n",
      "    features: ['word'],\n",
      "    num_rows: 4\n",
      "})\n",
      "tensor([[ 101, 1044, 1041, 1048, 1048,  103,  102,    0,    0,    0],\n",
      "        [ 101,  103, 1051, 1054, 1048,  103,  102,    0,    0,    0]])\n",
      "tensor([[-100, -100, -100, -100, -100,   14, -100, -100, -100, -100],\n",
      "        [-100,   22,   14, -100, -100,    3, -100, -100, -100, -100]])\n",
      "tensor([[1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "         0, 1]])\n"
     ]
    }
   ],
   "source": [
    "# TOY EXAMPLE\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the custom data collator\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer, mlm_probability=0.5, max_length=10)\n",
    "\n",
    "# Toy dataset\n",
    "toy_data = {\n",
    "    \"word\": [\"h e l l o\", \"w o r l d\", \"p a r i s\", \"b e r l i n\"]\n",
    "}\n",
    "df_temp = pd.DataFrame(toy_data)\n",
    "print(df_temp)\n",
    "\n",
    "# Tokenize the toy dataset\n",
    "TensorDataset = Dataset.from_pandas(df_temp)\n",
    "print(TensorDataset)\n",
    "toy_dataloader = DataLoader(TensorDataset, batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "# Print the first batch\n",
    "for batch in toy_dataloader:\n",
    "    print(batch['input_ids'])\n",
    "    print(batch['labels'])\n",
    "    print(batch['prev_guess'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.int64\n",
      "labels torch.int64\n",
      "prev_guess torch.int64\n",
      "input_ids tensor([[ 101, 1056, 1045,  103, 1052, 1037, 1050, 1045,  102,    0],\n",
      "        [ 101, 1059, 1051, 1054, 1055, 1048,  103,  102,    0,    0],\n",
      "        [ 101, 1061, 1045, 1050, 1055, 1056,  102,    0,    0,    0],\n",
      "        [ 101,  103, 1054, 1037, 1050,  103, 1041, 1054, 1045, 1062],\n",
      "        [ 101, 1049,  103, 1056,  103, 1056,  103, 1037,  102,    0],\n",
      "        [ 101,  103, 1045, 1055,  103, 1051,  103, 1045,  103,  103],\n",
      "        [ 101, 1054, 1041, 1039, 1051, 1048, 1048, 1041, 1039, 1056],\n",
      "        [ 101,  103, 1051,  103,  103, 1049, 1049, 1041,  103,  103],\n",
      "        [ 101, 1055, 1056,  103,  103,  103,  102,    0,    0,    0],\n",
      "        [ 101, 1055, 1041, 1054, 1045, 1039, 1051,  103,  103, 1054]]) torch.Size([64, 42])\n",
      "labels tensor([[-100, -100, -100,   12, -100,    0, -100, -100, -100, -100],\n",
      "        [-100, -100, -100,   17, -100, -100,    4, -100, -100, -100],\n",
      "        [-100, -100,    8, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    6, -100, -100,   13,    6,    4, -100, -100, -100],\n",
      "        [-100, -100,    0, -100,    0, -100,   20,    0, -100, -100],\n",
      "        [-100,    7, -100,   18,   19, -100,   17, -100,    4,   19],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    2, -100,   18,   24, -100, -100, -100,    3,    8],\n",
      "        [-100, -100, -100,    8,   13,   10, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100,    8,    2, -100,    2,    0, -100]]) torch.Size([64, 42])\n",
      "prev_guess tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 0, 0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 1, 0, 0, 1, 0, 1]]) torch.Size([64, 26])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the custom data collator\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer, mlm_probability=0.5, max_length=42)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    data['train'], \n",
    "    batch_size=64,\n",
    "    collate_fn = data_collator\n",
    ")\n",
    "\n",
    "# Prepare a batch using the custom data collator\n",
    "for batch in train_dataloader:\n",
    "    print(\"input_ids\", batch['input_ids'].dtype)\n",
    "    print(\"labels\", batch['labels'].dtype)\n",
    "    print(\"prev_guess\", batch['prev_guess'].dtype)\n",
    "\n",
    "    print(\"input_ids\", batch['input_ids'][:10,:10], batch['input_ids'].shape)\n",
    "    print(\"labels\", batch['labels'][:10,:10], batch['labels'].shape)\n",
    "    print(\"prev_guess\", batch['prev_guess'][:10,:10], batch['prev_guess'].shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9/9 [00:00<00:00, 1179.35 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'attention_mask', 'prev_guess'],\n",
      "    num_rows: 9\n",
      "})\n",
      "tensor([ 101, 1044, 1041,  103,  103, 1039, 1048,  103, 1056,  103, 1055,  102,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "tensor([-100, -100, -100,   17,    0, -100, -100,    8, -100,   20, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "torch.Size([9, 42])\n",
      "torch.Size([9, 42])\n",
      "torch.Size([9, 26])\n",
      "input_ids torch.int64\n",
      "labels torch.int64\n",
      "prev_guess torch.int64\n",
      "input_ids tensor([ 101, 1044, 1041,  103,  103, 1039, 1048,  103, 1056,  103, 1055,  102,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0]) torch.Size([3, 42])\n",
      "labels tensor([-100, -100, -100,   17,    0, -100, -100,    8, -100,   20, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100]) torch.Size([3, 42])\n",
      "prev_guess tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0]) torch.Size([3, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class FixedTokenizer:\n",
    "    def __init__(self, tokenizer, max_length=42):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.char_to_index = {chr(i + ord('a')): i for i in range(26)}\n",
    "\n",
    "    def __call__(self, examples):\n",
    "\n",
    "        masked_word = examples['masked_word']\n",
    "        labels_word = examples['labels']\n",
    "        prev_guesses = examples['previous_guesses']\n",
    "\n",
    "        # Create a labels tensor with all the values to -100\n",
    "        labels_batch = torch.full((len(masked_word),self.max_length,), -100, dtype=torch.int64)\n",
    "\n",
    "        for i in range(len(masked_word)):\n",
    "            # Replace the underscore in masked_word with the special [MASK] token\n",
    "            masked_word[i] = masked_word[i].replace('_', '[MASK]')\n",
    "\n",
    "        # Tokenize the masked_words\n",
    "        batch = self.tokenizer(masked_word, truncation=True, padding='max_length', return_tensors=\"pt\", max_length=self.max_length)\n",
    "\n",
    "        # Create the labels per word\n",
    "        for i in range(len(masked_word)):\n",
    "\n",
    "            # Split labels_word[i] into a list of characters considering they are separated by spaces\n",
    "            labels_word[i] = labels_word[i].split()\n",
    "            masked_word[i] = masked_word[i].split()\n",
    "\n",
    "\n",
    "            # Convert labels_word to their corresponding numerical labels\n",
    "            for j, char in enumerate(labels_word[i]):\n",
    "                if masked_word[i][j] == '[MASK]':\n",
    "                    labels_batch[i][j + 1] = self.char_to_index[char]\n",
    "\n",
    "        prev_guesses_batch = torch.tensor(prev_guesses, dtype=torch.int64)\n",
    "\n",
    "        batch['prev_guess'] = prev_guesses_batch\n",
    "        batch['labels'] = labels_batch\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example usage with first 9 word\n",
    "df = df_val[:9]\n",
    "\n",
    "data_set = Dataset.from_pandas(df)\n",
    "\n",
    "# Initialize the data collator\n",
    "tokenizer = FixedTokenizer(tokenizer, max_length=42)\n",
    "\n",
    "# Process the examples\n",
    "data_set = data_set.map(tokenizer, batched=True, batch_size=3)\n",
    "data_set.set_format(\"torch\")\n",
    "data_set = data_set.remove_columns(['masked_word','previous_guesses', 'token_type_ids'])\n",
    "\n",
    "print(data_set)\n",
    "\n",
    "print(data_set['input_ids'][0])\n",
    "print(data_set['labels'][0])\n",
    "print(data_set['prev_guess'][0])\n",
    "\n",
    "# Print the shapes of the processed data\n",
    "print(data_set['input_ids'].shape)\n",
    "print(data_set['labels'].shape)\n",
    "print(data_set['prev_guess'].shape)\n",
    "\n",
    "# Data loader\n",
    "val_dataloader = DataLoader(\n",
    "    data_set, \n",
    "    batch_size=3,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Check the first batch of the validation data loader\n",
    "for batch in val_dataloader:\n",
    "    print(\"input_ids\", batch['input_ids'].dtype)\n",
    "    print(\"labels\", batch['labels'].dtype)\n",
    "    print(\"prev_guess\", batch['prev_guess'].dtype)\n",
    "\n",
    "    print(\"input_ids\", batch['input_ids'][0], batch['input_ids'].shape)\n",
    "    print(\"labels\", batch['labels'][0], batch['labels'].shape)\n",
    "    print(\"prev_guess\", batch['prev_guess'][0], batch['prev_guess'].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22730/22730 [00:10<00:00, 2224.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Validation and Test datasets\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the data collator\n",
    "tokenizer = FixedTokenizer(tokenizer, max_length=42)\n",
    "\n",
    "\n",
    "# Tokenize the val dataset\n",
    "tokenized_val_data = data['valid'].map(tokenizer, batched=True, batch_size=3)\n",
    "tokenized_val_data.set_format(\"torch\")\n",
    "tokenized_val_data = tokenized_val_data.remove_columns(['masked_word','previous_guesses', 'token_type_ids'])\n",
    "\n",
    "# Tokenize the test dataset\n",
    "# tokenized_test_data = data['test'].map(tokenizer, batched=True, batch_size=3)\n",
    "# tokenized_test_data.set_format(\"torch\")\n",
    "# tokenized_test_data = tokenized_test_data.remove_columns(['masked_word','previous_guesses', 'token_type_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desing the model for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset,DatasetDict\n",
    "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanNet(nn.Module):\n",
    "  def __init__(self,checkpoint, vocab_size = 26, hidden_ffn_size = 410, unfreeze_layers = 0): \n",
    "    super(HangmanNet,self).__init__() \n",
    "    self.num_labels = vocab_size \n",
    "\n",
    "    #Load Model with given checkpoint and extract its body\n",
    "    self.model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "    \n",
    "    # Freeze all layers in the BERT model\n",
    "    for param in self.model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the last `unfreeze_layers` layers\n",
    "    if unfreeze_layers > 0:\n",
    "        for layer in self.model.encoder.layer[-unfreeze_layers:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Linear(768 + vocab_size, hidden_ffn_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_ffn_size, 26)\n",
    "    )\n",
    "    \n",
    "    # self.classifier = nn.Linear(768 + vocab_size,vocab_size) # load and initialize weights\n",
    "  \n",
    "  def forward(self, input_ids=None, attention_mask=None, labels=None, prev_guess=None,\n",
    "              token_type_ids=None):\n",
    "      outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "      \n",
    "      sequence_output = outputs.last_hidden_state  # (batch_size, sequence_length, hidden_size)\n",
    "      sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "      # Concatenate the previous guesses to the sequence_output\n",
    "      # (batch_size, sequence_length, hidden_size + vocab_size)\n",
    "      sequence_output = torch.cat((sequence_output, prev_guess.unsqueeze(1).repeat(1, sequence_output.shape[1], 1)), dim=2)\n",
    "\n",
    "      logits = self.classifier(sequence_output)  # (batch_size, sequence_length, num_labels)\n",
    "\n",
    "      # Mask the logits to zero out probabilities of previously guessed characters by considering the one-hot encoding in prev guesses\n",
    "      logits[prev_guess.unsqueeze(1).repeat(1, sequence_output.shape[1], 1) == 1] = -float(\"inf\")\n",
    "\n",
    "      loss = None\n",
    "      if labels is not None:\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "          # NOTE: I already has the labels in active logits representation\n",
    "\n",
    "          active_logits = logits.view(-1, self.num_labels)\n",
    "\n",
    "        #   active_labels = torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))\n",
    "          loss = loss_fct(active_logits, labels.view(-1))\n",
    "        \n",
    "      return TokenClassifierOutput(logits=logits, loss=loss, hidden_states=outputs.hidden_states,attentions=outputs.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8518, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([64, 42, 26])\n"
     ]
    }
   ],
   "source": [
    "# Check the model output\n",
    "device = \"cpu\"\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "model = HangmanNet(checkpoint=checkpoint, vocab_size = 26, unfreeze_layers = 1).to(device)\n",
    "\n",
    "# Prepare a batch using the custom data collator\n",
    "for batch in train_dataloader:\n",
    "\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "    prev_guess = batch['prev_guess']\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, prev_guess=prev_guess)\n",
    "    print(outputs.loss)\n",
    "    print(outputs.logits.shape)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "model.encoder.layer.10.attention.self.query.weight\n",
      "model.encoder.layer.10.attention.self.query.bias\n",
      "model.encoder.layer.10.attention.self.key.weight\n",
      "model.encoder.layer.10.attention.self.key.bias\n",
      "model.encoder.layer.10.attention.self.value.weight\n",
      "model.encoder.layer.10.attention.self.value.bias\n",
      "model.encoder.layer.10.attention.output.dense.weight\n",
      "model.encoder.layer.10.attention.output.dense.bias\n",
      "model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.10.intermediate.dense.weight\n",
      "model.encoder.layer.10.intermediate.dense.bias\n",
      "model.encoder.layer.10.output.dense.weight\n",
      "model.encoder.layer.10.output.dense.bias\n",
      "model.encoder.layer.10.output.LayerNorm.weight\n",
      "model.encoder.layer.10.output.LayerNorm.bias\n",
      "model.encoder.layer.11.attention.self.query.weight\n",
      "model.encoder.layer.11.attention.self.query.bias\n",
      "model.encoder.layer.11.attention.self.key.weight\n",
      "model.encoder.layer.11.attention.self.key.bias\n",
      "model.encoder.layer.11.attention.self.value.weight\n",
      "model.encoder.layer.11.attention.self.value.bias\n",
      "model.encoder.layer.11.attention.output.dense.weight\n",
      "model.encoder.layer.11.attention.output.dense.bias\n",
      "model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.11.intermediate.dense.weight\n",
      "model.encoder.layer.11.intermediate.dense.bias\n",
      "model.encoder.layer.11.output.dense.weight\n",
      "model.encoder.layer.11.output.dense.bias\n",
      "model.encoder.layer.11.output.LayerNorm.weight\n",
      "model.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.2.weight\n",
      "classifier.2.bias\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = HangmanNet(checkpoint=checkpoint, vocab_size = 26, unfreeze_layers = 2).to(device)\n",
    "\n",
    "# Print the trainable parameters of the model\n",
    "print(\"Trainable parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "lr = 0.00005\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "num_epochs = 100\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     \"linear\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=num_training_steps,\n",
    "# )\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # Check the scheduler\n",
    "# # Function to get learning rate at each step\n",
    "# def get_lr_at_each_step(lr_scheduler, num_training_steps):\n",
    "#     lrs = []\n",
    "#     for step in range(num_training_steps):\n",
    "#         lr_scheduler.step()  # Update the scheduler\n",
    "#         lrs.append(optimizer.param_groups[0]['lr'])  # Get the learning rate\n",
    "#     return lrs\n",
    "\n",
    "# # Get the learning rates\n",
    "# learning_rates = get_lr_at_each_step(lr_scheduler, num_training_steps)\n",
    "\n",
    "# # Plot the learning rates\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(learning_rates, label='Learning Rate')\n",
    "# plt.xlabel('Training Steps')\n",
    "# plt.ylabel('Learning Rate')\n",
    "# plt.title('Learning Rate Schedule')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "mlm_probability = 0.5\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer,\n",
    "                                         mlm_probability=mlm_probability,\n",
    "                                         max_length=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    data[\"train\"], \n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=True)\n",
    "\n",
    "# Probably add shuffle\n",
    "    \n",
    "eval_dataloader = DataLoader(tokenized_val_data,\n",
    "                             batch_size=64,\n",
    "                             num_workers=8,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. Anadir otra capa al clasifier + anadir al output la prior encoding info\n",
    "# 2. Eliminar el scheduler\n",
    "# 3. Descongelar otra capa\n",
    "# 3. añadir la otra prior information basada en frecuencias\n",
    "# 4. Set la probabilidad aleatoria en lugar de fija\n",
    "# 4. Añadir el early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"models/basic_models/model_lr_0.00005/model_epoch_15.pth\"\n",
    "# checkpoint = torch.load(checkpoint_path)\n",
    "# model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: models/basic_models/model_lr_5e-05_mlm_prob_0.5\n",
      "Loaded checkpoint from epoch: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 51/100: 100%|██████████| 3197/3197 [05:11<00:00, 10.27it/s]\n",
      "Validation Epoch 51/100: 100%|██████████| 356/356 [00:26<00:00, 13.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "Training Loss: 1.1668\n",
      "Validation Loss: 1.7397\n",
      "Validation Accuracy: 0.4152\n",
      "Validation F1 Score: 0.3532\n",
      "Learning Rate: 0.00005000\n",
      "Loss Change: 0.0000 (train), 0.0000 (val)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 52/100: 100%|██████████| 3197/3197 [05:29<00:00,  9.71it/s]\n",
      "Validation Epoch 52/100: 100%|██████████| 356/356 [00:27<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100\n",
      "Training Loss: 1.1721\n",
      "Validation Loss: 1.7380\n",
      "Validation Accuracy: 0.4166\n",
      "Validation F1 Score: 0.3555\n",
      "Learning Rate: 0.00005000\n",
      "Loss Change: 0.0000 (train), 0.0000 (val)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 53/100: 100%|██████████| 3197/3197 [05:31<00:00,  9.64it/s]\n",
      "Validation Epoch 53/100: 100%|██████████| 356/356 [00:27<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "Training Loss: 1.1709\n",
      "Validation Loss: 1.7395\n",
      "Validation Accuracy: 0.4151\n",
      "Validation F1 Score: 0.3541\n",
      "Learning Rate: 0.00005000\n",
      "Loss Change: 0.0000 (train), 0.0000 (val)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 54/100: 100%|██████████| 3197/3197 [05:30<00:00,  9.68it/s]\n",
      "Validation Epoch 54/100: 100%|██████████| 356/356 [00:27<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "Training Loss: 1.1722\n",
      "Validation Loss: 1.7387\n",
      "Validation Accuracy: 0.4160\n",
      "Validation F1 Score: 0.3561\n",
      "Learning Rate: 0.00005000\n",
      "Loss Change: 0.0000 (train), 0.0000 (val)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 55/100: 100%|██████████| 3197/3197 [05:29<00:00,  9.71it/s]\n",
      "Validation Epoch 55/100: 100%|██████████| 356/356 [00:27<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "Training Loss: 1.1681\n",
      "Validation Loss: 1.7404\n",
      "Validation Accuracy: 0.4166\n",
      "Validation F1 Score: 0.3565\n",
      "Learning Rate: 0.00005000\n",
      "Loss Change: 0.0000 (train), 0.0000 (val)\n",
      "Model saved to models/basic_models/model_lr_5e-05_mlm_prob_0.5/model_epoch_55.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 56/100: 100%|██████████| 3197/3197 [05:31<00:00,  9.66it/s]\n",
      "Validation Epoch 56/100: 100%|██████████| 356/356 [00:27<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "Training Loss: 1.1662\n",
      "Validation Loss: 1.7396\n",
      "Validation Accuracy: 0.4157\n",
      "Validation F1 Score: 0.3556\n",
      "Learning Rate: 0.00005000\n",
      "Loss Change: 0.0000 (train), 0.0000 (val)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 57/100: 100%|██████████| 3197/3197 [05:29<00:00,  9.70it/s]\n",
      "Validation Epoch 57/100: 100%|██████████| 356/356 [00:27<00:00, 13.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "Training Loss: 1.1641\n",
      "Validation Loss: 1.7349\n",
      "Validation Accuracy: 0.4180\n",
      "Validation F1 Score: 0.3571\n",
      "Learning Rate: 0.00005000\n",
      "Loss Change: 0.0000 (train), 0.0000 (val)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 58/100: 100%|██████████| 3197/3197 [05:30<00:00,  9.67it/s]\n",
      "Validation Epoch 58/100: 100%|██████████| 356/356 [00:26<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "Training Loss: 1.1653\n",
      "Validation Loss: 1.7362\n",
      "Validation Accuracy: 0.4174\n",
      "Validation F1 Score: 0.3568\n",
      "Learning Rate: 0.00005000\n",
      "Loss Change: 0.0000 (train), 0.0000 (val)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 59/100: 100%|██████████| 3197/3197 [05:30<00:00,  9.69it/s]\n",
      "Validation Epoch 59/100: 100%|██████████| 356/356 [00:27<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "Training Loss: 1.1621\n",
      "Validation Loss: 1.7345\n",
      "Validation Accuracy: 0.4173\n",
      "Validation F1 Score: 0.3590\n",
      "Learning Rate: 0.00005000\n",
      "Loss Change: 0.0000 (train), 0.0000 (val)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 60/100: 100%|██████████| 3197/3197 [05:30<00:00,  9.69it/s]\n",
      "Validation Epoch 60/100: 100%|██████████| 356/356 [00:26<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "Training Loss: 1.1616\n",
      "Validation Loss: 1.7354\n",
      "Validation Accuracy: 0.4170\n",
      "Validation F1 Score: 0.3589\n",
      "Learning Rate: 0.00005000\n",
      "Loss Change: 0.0000 (train), 0.0000 (val)\n",
      "Model saved to models/basic_models/model_lr_5e-05_mlm_prob_0.5/model_epoch_60.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 61/100: 100%|██████████| 3197/3197 [05:30<00:00,  9.68it/s]\n",
      "Validation Epoch 61/100: 100%|██████████| 356/356 [00:27<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "Training Loss: 1.1605\n",
      "Validation Loss: 1.7339\n",
      "Validation Accuracy: 0.4161\n",
      "Validation F1 Score: 0.3572\n",
      "Learning Rate: 0.00005000\n",
      "Loss Change: 0.0000 (train), 0.0000 (val)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 62/100:  19%|█▉        | 620/3197 [01:04<04:27,  9.64it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# lr_scheduler.step()\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 56\u001b[0m     total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m     59\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "save_dir = \"models/basic_models\"\n",
    "save_dir = os.path.join(save_dir, f\"model_lr_{lr}_mlm_prob_{mlm_probability}\")\n",
    "print(f\"Saving model to: {save_dir}\")\n",
    "writer = SummaryWriter(log_dir=save_dir)\n",
    "\n",
    "# Initialize metrics storage\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_f1_scores = []\n",
    "\n",
    "# Define the save frequency\n",
    "save_frequency = 5  # Save model every n epochs, adjust this as needed\n",
    "\n",
    "# Load the model checkpoint if available\n",
    "checkpoint_path = \"models/basic_models/model_lr_5e-05_mlm_prob_0.5/model_epoch_50.pth\"  # Provide the path to your saved checkpoint\n",
    "start_epoch = 0\n",
    "\n",
    "prev_val_loss = 1000\n",
    "prev_train_loss = 1000\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    val_accuracies = checkpoint['val_accuracies']\n",
    "    val_f1_scores = checkpoint['val_f1_scores']\n",
    "    print(f\"Loaded checkpoint from epoch: {start_epoch}\")\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        # lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        flat_predictions = predictions.view(-1)\n",
    "        flat_references = batch[\"labels\"].view(-1)\n",
    "        flat_attention_mask = batch[\"labels\"].view(-1) != -100\n",
    "\n",
    "        active_predictions = flat_predictions[flat_attention_mask]\n",
    "        active_references = flat_references[flat_attention_mask]\n",
    "\n",
    "        all_predictions.extend(active_predictions.cpu().numpy())\n",
    "        all_references.extend(active_references.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(eval_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    val_accuracy = accuracy_score(all_references, all_predictions)\n",
    "    val_f1 = f1_score(all_references, all_predictions, average=\"macro\")\n",
    "\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalars('Loss', {'train_loss': avg_train_loss, 'val_loss' : avg_val_loss}, epoch)\n",
    "    writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "    writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "    writer.add_scalars('Loss Change: ', {'train_loss_change': prev_train_loss - avg_train_loss, \n",
    "                                        'val_loss_change': prev_val_loss - avg_val_loss}, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.8f}\")\n",
    "    print(f\"Loss Change: {prev_train_loss - avg_train_loss:.4f} (train), {prev_val_loss - avg_val_loss:.4f} (val)\")\n",
    "\n",
    "    prev_train_loss = avg_train_loss\n",
    "    prev_val_loss = avg_val_loss  \n",
    "\n",
    "    # Save the model every n epochs\n",
    "    if (epoch + 1) % save_frequency == 0:\n",
    "        model_save_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pth\")\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'val_f1_scores': val_f1_scores\n",
    "        }\n",
    "        torch.save(checkpoint, model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save the final model\n",
    "model_save_path = os.path.join(save_dir, \"model_final.pth\")\n",
    "checkpoint = {\n",
    "    'epoch': epoch + 1,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'val_f1_scores': val_f1_scores\n",
    "}\n",
    "torch.save(checkpoint, model_save_path)\n",
    "print(f\"Final model saved to {model_save_path}\")\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning and CharacterPredictor Head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130436/130436 [00:59<00:00, 2198.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the data collator\n",
    "tokenizer = FixedTokenizer(tokenizer, max_length=42)\n",
    "\n",
    "# Tokenize the test dataset\n",
    "tokenized_test_data = data['test'].map(tokenizer, batched=True, batch_size=3)\n",
    "tokenized_test_data.set_format(\"torch\")\n",
    "tokenized_test_data = tokenized_test_data.remove_columns(['masked_word','previous_guesses', 'token_type_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = HangmanNet(checkpoint=model_name, vocab_size = 26, unfreeze_layers = 2).to(device)\n",
    "\n",
    "# Load the model an perform an inference\n",
    "checkpoint_path = \"models/basic_models/model_lr_5e-05_mlm_prob_0.5/model_epoch_45.pth\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Prior Information:  torch.Size([28, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2039/2039 [04:17<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4303\n",
      "Test F1 Score: 0.3679\n",
      "Test Accuracy Unique Char: 0.6849\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Data loader\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_test_data,\n",
    "    num_workers=8,\n",
    "    batch_size=64,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "def accuracy_unique_char(logits, labels, mode = 'greedy', attention_mask = None, prior_probs = None, alpha_prior = 0.5):\n",
    "    batch_size, seq_length, num_classes = logits.shape\n",
    "    correct_predictions = 0\n",
    "\n",
    "    if attention_mask is not None:\n",
    "        word_lengths = attention_mask.sum(dim=1) - 2\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Extract the logits and labels for the current sequence\n",
    "        logits_seq = logits[i]\n",
    "        labels_seq = labels[i]\n",
    "\n",
    "        # Identify the indices of active tokens (not -100)\n",
    "        active_indices = labels_seq != -100\n",
    "\n",
    "        # Get the logits of the active tokens\n",
    "        active_logits = logits_seq[active_indices]\n",
    "\n",
    "        if active_logits.shape[0] == 0:\n",
    "            continue  # skip sequences with no active tokens\n",
    "\n",
    "        # Get the class with the highest probability among active tokens\n",
    "        max_prob_class = torch.argmax(active_logits, dim=-1)\n",
    "\n",
    "        # Get the actual labels of the active tokens\n",
    "        active_labels = labels_seq[active_indices].unique()\n",
    "\n",
    "        if mode == 'greedy':\n",
    "            # Get the probabilities of the active tokens\n",
    "            max_prob = torch.max(active_logits, dim=-1).values\n",
    "\n",
    "            # Get the index of the maximum probability\n",
    "            max_prob_index = torch.argmax(max_prob)\n",
    "\n",
    "            # Take a greedy choose and take the largest probability\n",
    "            max_prob_char = max_prob_class[max_prob_index]\n",
    "\n",
    "            if max_prob_char in active_labels:\n",
    "                correct_predictions += 1\n",
    "\n",
    "        elif mode == 'random':\n",
    "            # Double sampling process\n",
    "            \n",
    "            # Convert logits to probabilities\n",
    "            probabilities = F.softmax(active_logits, dim=-1)\n",
    "            \n",
    "            # Sample from the probability distribution\n",
    "            sampled_classes = torch.multinomial(probabilities, num_samples=1).squeeze()\n",
    "\n",
    "            # NOTE: There is only one posibility\n",
    "            if sampled_classes.dim() == 0:\n",
    "                max_prob_char = sampled_classes\n",
    "\n",
    "            else:\n",
    "                # Get the logits for each sampled class\n",
    "                sampled_logits = active_logits.gather(1, sampled_classes.unsqueeze(-1)).squeeze()\n",
    "\n",
    "                # Convert logits to probabilities\n",
    "                probabilities_sampled_logits = F.softmax(sampled_logits, dim=-1)\n",
    "\n",
    "                # Among all samples, take the one with the highest probability\n",
    "                max_prob_index = torch.multinomial(probabilities_sampled_logits, num_samples=1).squeeze()\n",
    "\n",
    "                max_prob_char = sampled_classes[max_prob_index]\n",
    "\n",
    "            if max_prob_char in active_labels:\n",
    "                correct_predictions += 1\n",
    "\n",
    "        elif mode == 'random_greedy':\n",
    "            # First one random pick and then a greedy pick with argmax\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probabilities = F.softmax(active_logits, dim=-1)\n",
    "            \n",
    "            # Sample from the probability distribution\n",
    "            sampled_classes = torch.multinomial(probabilities, num_samples=1).squeeze()\n",
    "\n",
    "            # NOTE: There is only one posibility\n",
    "            if sampled_classes.dim() == 0:\n",
    "                max_prob_char = sampled_classes\n",
    "            else:\n",
    "                # Get the probability for each sampled class\n",
    "                sampled_probs = probabilities.gather(1, sampled_classes.unsqueeze(-1)).squeeze()\n",
    "\n",
    "                # Among all samples, take the one with the highest probability\n",
    "                max_prob_index = torch.argmax(sampled_probs)\n",
    "\n",
    "                max_prob_char = sampled_classes[max_prob_index]\n",
    "\n",
    "            if max_prob_char in active_labels:\n",
    "                correct_predictions += 1\n",
    "        \n",
    "        elif mode == 'greedy_random':\n",
    "            # First one random pick and then a greedy pick with argmax\n",
    "\n",
    "            # Get the character id with the maximum probability\n",
    "            max_prob_classes = torch.argmax(active_logits, dim=-1)\n",
    "\n",
    "            # Get the largest logit corresponding to the character id\n",
    "            max_logits = torch.max(active_logits, dim=-1).values\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probabilities_sampled_logits = F.softmax(max_logits, dim=-1)\n",
    "\n",
    "            # Among all samples, take the one with the highest probability\n",
    "            max_prob_index = torch.multinomial(probabilities_sampled_logits, num_samples=1).squeeze()\n",
    "\n",
    "            max_prob_char = max_prob_classes[max_prob_index]\n",
    "\n",
    "            if max_prob_char in active_labels:\n",
    "                correct_predictions += 1\n",
    "\n",
    "        elif (attention_mask is not None) and mode == 'greedy_random_prior':\n",
    "            # Get the character id with the maximum probability\n",
    "            max_prob_classes = torch.argmax(active_logits, dim=-1)\n",
    "\n",
    "            # Get the largest logit corresponding to the character id\n",
    "            max_logits = torch.max(active_logits, dim=-1).values\n",
    "\n",
    "            # Get the prior probabilities for the active tokens\n",
    "            word_len = word_lengths[i] - 1\n",
    "            prior_probs_seq = prior_probs[word_len][max_prob_classes]\n",
    "\n",
    "            sum_logits = max_logits + alpha_prior * prior_probs_seq\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probabilities_sampled_logits = F.softmax(sum_logits, dim=-1)            \n",
    "\n",
    "            # Among all samples, take the one with the highest probability\n",
    "            max_prob_index = torch.multinomial(probabilities_sampled_logits, num_samples=1).squeeze()\n",
    "\n",
    "            max_prob_char = max_prob_classes[max_prob_index]\n",
    "\n",
    "            if max_prob_char in active_labels:\n",
    "                correct_predictions += 1\n",
    "\n",
    "\n",
    "        elif mode == 'all':\n",
    "            # Check if the predicted class is in the actual labels\n",
    "            if any(label in active_labels for label in max_prob_class.unique()):\n",
    "                correct_predictions += 1\n",
    "\n",
    "    return correct_predictions\n",
    "\n",
    "# Perform an inference\n",
    "all_predictions = []\n",
    "all_references = []\n",
    "\n",
    "# Intialize to calculate our new metric\n",
    "correct_predictions = 0\n",
    "total_sequences = 0\n",
    "\n",
    "# Load the prior probabilities\n",
    "prior_probs = torch.tensor(pd.read_csv(\"data/total_rel_freq.csv\").to_numpy(), dtype=torch.float32).to(device)\n",
    "print(\"Loaded Prior Information: \", prior_probs.shape)\n",
    "\n",
    "model.eval()\n",
    "for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    flat_predictions = predictions.view(-1)\n",
    "    flat_references = batch[\"labels\"].view(-1)\n",
    "    flat_attention_mask = batch[\"labels\"].view(-1) != -100\n",
    "\n",
    "    active_predictions = flat_predictions[flat_attention_mask] # [4, 8, 9]\n",
    "    active_references = flat_references[flat_attention_mask] # [7, 2, 8]\n",
    "\n",
    "    all_predictions.extend(active_predictions.cpu().numpy())\n",
    "    all_references.extend(active_references.cpu().numpy())\n",
    "\n",
    "    # Calculate the custom accuracy\n",
    "    accuracy_hangman = accuracy_unique_char(logits, batch[\"labels\"], \n",
    "                                            mode='greedy', \n",
    "                                            attention_mask=batch['attention_mask'],\n",
    "                                            prior_probs=prior_probs,\n",
    "                                            alpha_prior=1)\n",
    "    correct_predictions += accuracy_hangman\n",
    "    total_sequences += logits.shape[0]\n",
    "\n",
    "# Compute the accuracy and F1 score\n",
    "test_accuracy = accuracy_score(all_references, all_predictions)\n",
    "test_f1 = f1_score(all_references, all_predictions, average=\"macro\")\n",
    "test_accuracy_unique_char = correct_predictions / total_sequences\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test Accuracy Unique Char: {test_accuracy_unique_char:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random: Test Accuracy Unique Char: 0.6274\n",
    "# greedy: Test Accuracy Unique Char: 0.7044\n",
    "# random_greedy: Test Accuracy Unique Char: 0.6698\n",
    "# greedy_random: Test Accuracy Unique Char: 0.6847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://stackoverflow.com/questions/69907682/what-are-differences-between-automodelforsequenceclassification-vs-automodel\n",
    "\n",
    "https://huggingface.co/learn/nlp-course/chapter3/4?fw=pt\n",
    "\n",
    "https://aclanthology.org/2020.coling-main.609.pdf\n",
    "\n",
    "https://github.com/helboukkouri/character-bert?tab=readme-ov-file#how-do-i-pre-train-characterbert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "character-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
