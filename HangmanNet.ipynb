{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs\n",
    "# _ _ _ _ _\n",
    "\n",
    "# 250000 words\n",
    "\n",
    "# El potter no [MASK] novia\n",
    "\n",
    "# p _ t t e r\n",
    "\n",
    "\n",
    "# 1. Filtrar las palabras\n",
    "# 2. Palabras mas comunes\n",
    "# 3. Data colador\n",
    "# 4. Penalize more small words??\n",
    "# 5. Podria concatenar las palabras adivinadas hasta ahora al final del classifier, es practicamente concatenar un one-hot vector \n",
    "# de 0-26 con 1 donde las palabras ya fueron adivinadas.\n",
    "# 6. Podria usar one-hot encodings 0-27 para los characteres\n",
    "# 7. El Test y el val tambien generalo de manera dinamica. No te preocupes por que distribucion tendra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word  word_length  unique_chars\n",
      "0      timpani            7             6\n",
      "1       worsle            6             6\n",
      "2        yinst            5             5\n",
      "3  grangerized           11             8\n",
      "4      matatua            7             4\n",
      "(204570, 3)\n",
      "(22730, 3)\n",
      "(130436, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets \n",
    "df_train = pd.read_csv('data/to_mask_train.csv', converters={'word': str})\n",
    "df_val = pd.read_csv('data/masked_val.csv', converters={'masked_word': str, 'labels': str, 'previous_guesses': eval})\n",
    "df_test = pd.read_csv('data/masked_test.csv', converters={'masked_word': str, 'labels': str, 'previous_guesses': eval})\n",
    "\n",
    "print(df_train.head())\n",
    "\n",
    "# Add spaces to the word in training\n",
    "df_train[\"word\"] = df_train[\"word\"].apply(lambda x: ' '.join(list(x)))\n",
    "\n",
    "# Add spaces to the masked_word and labels\n",
    "df_val[\"masked_word\"] = df_val[\"masked_word\"].apply(lambda x: ' '.join(list(x)))\n",
    "df_val[\"labels\"] = df_val[\"labels\"].apply(lambda x: ' '.join(list(x)))\n",
    "df_test[\"masked_word\"] = df_test[\"masked_word\"].apply(lambda x: ' '.join(list(x)))\n",
    "df_test[\"labels\"] = df_test[\"labels\"].apply(lambda x: ' '.join(list(x)))\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>word_length</th>\n",
       "      <th>unique_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t i m p a n i</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>w o r s l e</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>y i n s t</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g r a n g e r i z e d</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m a t a t u a</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    word  word_length  unique_chars\n",
       "0          t i m p a n i            7             6\n",
       "1            w o r s l e            6             6\n",
       "2              y i n s t            5             5\n",
       "3  g r a n g e r i z e d           11             8\n",
       "4          m a t a t u a            7             4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_word</th>\n",
       "      <th>labels</th>\n",
       "      <th>previous_guesses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h e _ _ c l _ t _ s</td>\n",
       "      <td>h e r a c l i t u s</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_ e _ o d _ _ _</td>\n",
       "      <td>m e r o d a c h</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_ _ g _ _ _</td>\n",
       "      <td>i n g i r t</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_ _ _ z u o k a</td>\n",
       "      <td>s h i z u o k a</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m _ _ t _ _ _ _ n n e _ _ e d</td>\n",
       "      <td>m u l t i c h a n n e l l e d</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     masked_word                         labels  \\\n",
       "0            h e _ _ c l _ t _ s            h e r a c l i t u s   \n",
       "1                _ e _ o d _ _ _                m e r o d a c h   \n",
       "2                    _ _ g _ _ _                    i n g i r t   \n",
       "3                _ _ _ z u o k a                s h i z u o k a   \n",
       "4  m _ _ t _ _ _ _ n n e _ _ e d  m u l t i c h a n n e l l e d   \n",
       "\n",
       "                                    previous_guesses  \n",
       "0  [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...  \n",
       "4  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['word'],\n",
       "        num_rows: 204570\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['masked_word', 'labels', 'previous_guesses'],\n",
       "        num_rows: 22730\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['masked_word', 'labels', 'previous_guesses'],\n",
       "        num_rows: 130436\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather all the data in a DatasetDict\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train[[\"word\"]]),\n",
    "    \"valid\": Dataset.from_pandas(df_val),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling, BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataCollatorForMLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer, mlm_probability=0.75, max_length=42):\n",
    "        super().__init__(tokenizer=tokenizer, mlm_probability=mlm_probability)\n",
    "        self.max_length = max_length\n",
    "        self.char_to_index = {chr(i + ord('a')): i for i in range(26)}\n",
    "\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "\n",
    "        # Convert characters to their corresponding numerical labels\n",
    "        labels = []\n",
    "        words = []\n",
    "        for example in examples:\n",
    "            example = example['word']\n",
    "            row_label = [self.char_to_index[char] for char in example.split()]\n",
    "            labels.append(row_label)\n",
    "            words.append(example)\n",
    "\n",
    "        # Pad labels to the same length (considering special tokens at the beginning and end of each label)\n",
    "        labels = [ [-100] + row_label + [-100] * (self.max_length - len(row_label) - 1) for row_label in labels]\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # Create a one-hot vector for the labels without consider the -100 values\n",
    "        prev_guess = torch.zeros((labels.shape[0], 26))\n",
    "        for i, label in enumerate(labels):\n",
    "            prev_guess[i][label[label != -100]] = 1\n",
    "\n",
    "        # Tokenize and pad the input examples\n",
    "        batch = self.tokenizer(words, truncation=True, padding='max_length', return_tensors=\"pt\", max_length=self.max_length)\n",
    "        \n",
    "        # Get the input_ids and apply masking\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        # labels = input_ids.clone()\n",
    "\n",
    "        for i, input_id in enumerate(input_ids):\n",
    "\n",
    "            # Get unique tokens\n",
    "            unique_tokens = torch.unique(input_id)\n",
    "            \n",
    "            # Filter Special Tokens by setting probability to 0.0\n",
    "            special_tokens_mask = self.tokenizer.get_special_tokens_mask(unique_tokens, already_has_special_tokens=True)\n",
    "            probabilities = torch.full(unique_tokens.shape, self.mlm_probability)\n",
    "            probabilities.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "            \n",
    "            # Filter the tokens to mask\n",
    "            tokens_to_mask = unique_tokens[torch.bernoulli(probabilities).bool()]\n",
    "\n",
    "            # Mask all instances of the chosen tokens\n",
    "            masked_indices = torch.zeros(input_id.shape, dtype=torch.bool)\n",
    "            for token in tokens_to_mask:\n",
    "                masked_indices[(input_id == token)] = True\n",
    "\n",
    "            labels[i][~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "            # 80% of the time, replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "            indices_replaced = torch.bernoulli(torch.full(input_id.shape, 0.8)).bool() & masked_indices\n",
    "            input_ids[i][indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "            # 10% of the time, replace masked input tokens with random word\n",
    "            # indices_random = torch.bernoulli(torch.full(input_id.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "            # random_words = torch.randint(len(self.tokenizer), input_id.shape, dtype=torch.long)\n",
    "            # input_ids[i][indices_random] = random_words[indices_random]\n",
    "\n",
    "            # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "\n",
    "        # Set to zero the values masked in the labels\n",
    "        for i, label in enumerate(labels):\n",
    "            prev_guess[i][label[label != -100]] = 0\n",
    "\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"prev_guess\"] = prev_guess\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word\n",
      "0    h e l l o\n",
      "1    w o r l d\n",
      "2    p a r i s\n",
      "3  b e r l i n\n",
      "Dataset({\n",
      "    features: ['word'],\n",
      "    num_rows: 4\n",
      "})\n",
      "tensor([[ 101, 1044, 1041,  103, 1048, 1051,  102,    0,    0,    0],\n",
      "        [ 101, 1059, 1051, 1054,  103, 1040,  102,    0,    0,    0]])\n",
      "tensor([[-100, -100, -100,   11,   11, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100,   11, -100, -100, -100, -100, -100]])\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# TOY EXAMPLE\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the custom data collator\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer, mlm_probability=0.5, max_length=10)\n",
    "\n",
    "# Toy dataset\n",
    "toy_data = {\n",
    "    \"word\": [\"h e l l o\", \"w o r l d\", \"p a r i s\", \"b e r l i n\"]\n",
    "}\n",
    "df_temp = pd.DataFrame(toy_data)\n",
    "print(df_temp)\n",
    "\n",
    "# Tokenize the toy dataset\n",
    "TensorDataset = Dataset.from_pandas(df_temp)\n",
    "print(TensorDataset)\n",
    "toy_dataloader = DataLoader(TensorDataset, batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "# Print the first batch\n",
    "for batch in toy_dataloader:\n",
    "    print(batch['input_ids'])\n",
    "    print(batch['labels'])\n",
    "    print(batch['prev_guess'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[ 101,  103, 1045, 1049, 1052, 1037, 1050, 1045,  102,    0],\n",
      "        [ 101, 1059,  103,  103, 1055,  103,  103,  102,    0,    0],\n",
      "        [ 101, 1061, 1045,  103, 1055, 1056,  102,    0,    0,    0],\n",
      "        [ 101,  103, 1054,  103, 1050,  103, 1041, 1054,  103, 1062],\n",
      "        [ 101, 1049,  103, 1056,  103, 1056, 1057, 1037,  102,    0],\n",
      "        [ 101,  103, 1045, 1055, 1056,  103, 1054, 1045, 1041, 1056],\n",
      "        [ 101,  103,  103, 1039, 1051, 1048, 1048,  103, 1039, 1056],\n",
      "        [ 101, 1039,  103,  103, 1061,  103,  103, 1041, 1040, 1045],\n",
      "        [ 101,  103,  103, 1045, 1050,  103,  102,    0,    0,    0],\n",
      "        [ 101,  103, 1041, 1054, 1045,  103,  103, 1039, 1037, 1054]]) torch.Size([64, 42])\n",
      "labels tensor([[-100,   19, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100,   14,   17, -100,   11,    4, -100, -100, -100],\n",
      "        [-100, -100, -100,   13, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    6, -100,    0, -100,    6, -100, -100,    8, -100],\n",
      "        [-100, -100,    0, -100,    0, -100, -100,    0, -100, -100],\n",
      "        [-100,    7, -100, -100, -100,   14,   17, -100,    4, -100],\n",
      "        [-100,   17,    4, -100, -100, -100, -100,    4, -100, -100],\n",
      "        [-100, -100,   14,   18, -100,   12,   12, -100, -100, -100],\n",
      "        [-100,   18,   19, -100, -100,   10, -100, -100, -100, -100],\n",
      "        [-100,   18, -100,   17, -100,    2,   14,    2, -100,   17]]) torch.Size([64, 42])\n",
      "prev_guess tensor([[1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 1., 1., 1., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 0., 1., 0.]]) torch.Size([64, 26])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the custom data collator\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer, mlm_probability=0.5, max_length=42)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    data['train'], \n",
    "    batch_size=64,\n",
    "    collate_fn = data_collator\n",
    ")\n",
    "\n",
    "# Prepare a batch using the custom data collator\n",
    "for batch in train_dataloader:\n",
    "\n",
    "    print(\"input_ids\", batch['input_ids'][:10,:10], batch['input_ids'].shape)\n",
    "    print(\"labels\", batch['labels'][:10,:10], batch['labels'].shape)\n",
    "    print(\"prev_guess\", batch['prev_guess'][:10,:10], batch['prev_guess'].shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desing the model for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset,DatasetDict\n",
    "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanNet(nn.Module):\n",
    "  def __init__(self,checkpoint, vocab_size = 26, unfreeze_layers = 0): \n",
    "    super(HangmanNet,self).__init__() \n",
    "    self.num_labels = vocab_size \n",
    "\n",
    "    #Load Model with given checkpoint and extract its body\n",
    "    self.model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "    \n",
    "    # Freeze all layers in the BERT model\n",
    "    for param in self.model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the last `unfreeze_layers` layers\n",
    "    if unfreeze_layers > 0:\n",
    "        for layer in self.model.encoder.layer[-unfreeze_layers:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "\n",
    "    self.dropout = nn.Dropout(0.1) \n",
    "    self.classifier = nn.Linear(768,vocab_size) # load and initialize weights\n",
    "  \n",
    "\n",
    "  def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "      outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "      \n",
    "      sequence_output = outputs.last_hidden_state  # (batch_size, sequence_length, hidden_size)\n",
    "      sequence_output = self.dropout(sequence_output)\n",
    "      logits = self.classifier(sequence_output)  # (batch_size, sequence_length, num_labels)\n",
    "\n",
    "      loss = None\n",
    "      if labels is not None:\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "          # NOTE: I already has the labels in active logits representation\n",
    "\n",
    "          # Only keep active parts of the loss\n",
    "        #   active_loss = labels.view(-1) != -100\n",
    "          active_logits = logits.view(-1, self.num_labels)\n",
    "        #   active_labels = torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))\n",
    "          loss = loss_fct(active_logits, labels.view(-1))\n",
    "        \n",
    "      return TokenClassifierOutput(logits=logits, loss=loss, hidden_states=outputs.hidden_states,attentions=outputs.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2538, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([64, 42, 26])\n"
     ]
    }
   ],
   "source": [
    "# Check the model output\n",
    "device = \"cpu\"\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "model = HangmanNet(checkpoint=checkpoint, vocab_size = 26, unfreeze_layers = 1).to(device)\n",
    "\n",
    "# Prepare a batch using the custom data collator\n",
    "for batch in train_dataloader:\n",
    "\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    print(outputs.loss)\n",
    "    print(outputs.logits.shape)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "model.encoder.layer.11.attention.self.query.weight\n",
      "model.encoder.layer.11.attention.self.query.bias\n",
      "model.encoder.layer.11.attention.self.key.weight\n",
      "model.encoder.layer.11.attention.self.key.bias\n",
      "model.encoder.layer.11.attention.self.value.weight\n",
      "model.encoder.layer.11.attention.self.value.bias\n",
      "model.encoder.layer.11.attention.output.dense.weight\n",
      "model.encoder.layer.11.attention.output.dense.bias\n",
      "model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.11.intermediate.dense.weight\n",
      "model.encoder.layer.11.intermediate.dense.bias\n",
      "model.encoder.layer.11.output.dense.weight\n",
      "model.encoder.layer.11.output.dense.bias\n",
      "model.encoder.layer.11.output.LayerNorm.weight\n",
      "model.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HangmanNet(checkpoint=checkpoint, vocab_size = 26, unfreeze_layers = 1).to(device)\n",
    "\n",
    "# Print the trainable parameters of the model\n",
    "print(\"Trainable parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"f1\")\n",
    "metric_acc = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    data[\"train\"], \n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=True)\n",
    "    \n",
    "    # collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(data[\"valid\"],\n",
    "                             batch_size=64,\n",
    "                             num_workers=8,\n",
    "                             pin_memory=True)\n",
    "                             # collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/3:   0%|          | 0/4688 [00:00<?, ?it/s]BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Training Epoch 1/3: 100%|██████████| 4688/4688 [06:26<00:00, 12.12it/s]\n",
      "Validation Epoch 1/3: 100%|██████████| 782/782 [00:57<00:00, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Training Loss: 2.3355\n",
      "Validation Loss: 2.7463\n",
      "Validation Accuracy: 0.1691\n",
      "Validation F1 Score: 0.0858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/3:  23%|██▎       | 1093/4688 [01:33<05:07, 11.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     21\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 22\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     24\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 25\u001b[0m, in \u001b[0;36mHangmanNet.forward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state  \u001b[38;5;66;03m# (batch_size, sequence_length, hidden_size)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1137\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1137\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1150\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:690\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    679\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    680\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    681\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m         output_attentions,\n\u001b[1;32m    688\u001b[0m     )\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 690\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:622\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    619\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    620\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 622\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/transformers/pytorch_utils.py:238\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:634\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 634\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:534\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 534\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m(hidden_states)\n\u001b[1;32m    535\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/character-bert/lib/python3.10/site-packages/torch/nn/modules/module.py:1696\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1698\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir='tensorboard_logs')\n",
    "\n",
    "# Initialize metrics storage\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_f1_scores = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        flat_predictions = predictions.view(-1)\n",
    "        flat_references = batch[\"labels\"].view(-1)\n",
    "        flat_attention_mask = batch[\"labels\"].view(-1) < NO_CLASS\n",
    "\n",
    "        active_predictions = flat_predictions[flat_attention_mask]\n",
    "        active_references = flat_references[flat_attention_mask]\n",
    "\n",
    "        all_predictions.extend(active_predictions.cpu().numpy())\n",
    "        all_references.extend(active_references.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(eval_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    val_accuracy = accuracy_score(all_references, all_predictions)\n",
    "    val_f1 = f1_score(all_references, all_predictions, average=\"macro\")\n",
    "\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "save_dir = \"model_and_logs\"\n",
    "model_save_path = os.path.join(save_dir, \"model.pth\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  16%|█▌        | 1899/11721 [20:35<1:46:30,  1.54it/s]\n",
      "Evaluation Progress:   0%|          | 0/2346 [20:35<?, ?it/s]\n",
      "Training Progress:  33%|███▎      | 3905/11721 [03:59<08:08, 15.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 2.0151631832122803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  33%|███▎      | 3907/11721 [04:16<08:08, 15.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score: 0.19989245780120005\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMacro F1 Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     53\u001b[0m result_acc \u001b[38;5;241m=\u001b[39m metric_acc\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mactive_predictions, references\u001b[38;5;241m=\u001b[39mactive_references)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mresult_acc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43macc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# # You can also calculate the F1 score with micro and weighted averaging\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# result_micro = metric.compute(predictions=active_predictions, references=active_references, average=\"micro\")\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# print(\"Micro F1 Score:\", result_micro['f1'])\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# result_weighted = metric.compute(predictions=active_predictions, references=active_references, average=\"weighted\")\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# print(\"Weighted F1 Score:\", result_weighted['f1'])\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar_train = tqdm(range(num_training_steps), desc=\"Training Progress\")\n",
    "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)), desc=\"Evaluation Progress\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  for batch in train_dataloader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      lr_scheduler.step()\n",
    "      optimizer.zero_grad()\n",
    "      progress_bar_train.update(1)\n",
    "  \n",
    "  print(\"Epoch:\", epoch, \"Loss:\", loss.item())\n",
    "\n",
    "  model.eval()\n",
    "  for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    # Logits Size: (batch_size, sequence_length, num_labels)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Predictions: (batch_size, sequence_length)\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "    # Flatten the predictions, references, and attention mask\n",
    "    flat_predictions = predictions.view(-1)\n",
    "    flat_references = batch[\"labels\"].view(-1)\n",
    "    flat_attention_mask = batch[\"labels\"].view(-1) < NO_CLASS\n",
    "\n",
    "    # Filter out the padding tokens\n",
    "    active_predictions = flat_predictions[flat_attention_mask]\n",
    "    active_references = flat_references[flat_attention_mask]\n",
    "\n",
    "    metric.add_batch(predictions=active_predictions, references=active_references)\n",
    "\n",
    "    metric_acc.add_batch(predictions=active_predictions, references=active_references)\n",
    "  \n",
    "    progress_bar_eval.update(1)\n",
    "    \n",
    "  # Calculate the F1 score\n",
    "  result = metric.compute(predictions=active_predictions, references=active_references, average=\"macro\")\n",
    "  print(\"Macro F1 Score:\", result['f1'])\n",
    "\n",
    "  result_acc = metric_acc.compute(predictions=active_predictions, references=active_references)\n",
    "  print(\"Accuracy:\", result_acc['accuracy'])\n",
    "\n",
    "  # # You can also calculate the F1 score with micro and weighted averaging\n",
    "  # result_micro = metric.compute(predictions=active_predictions, references=active_references, average=\"micro\")\n",
    "  # print(\"Micro F1 Score:\", result_micro['f1'])\n",
    "\n",
    "  # result_weighted = metric.compute(predictions=active_predictions, references=active_references, average=\"weighted\")\n",
    "  # print(\"Weighted F1 Score:\", result_weighted['f1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning and CharacterPredictor Head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CharacterPredictor(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size):\n",
    "        super(CharacterPredictor, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Example: Assume `vocab_size` is the size of your character vocabulary\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "embedding_dim = embeddings.shape[-1]\n",
    "predictor = CharacterPredictor(embedding_dim, vocab_size)\n",
    "\n",
    "# Predict masked characters\n",
    "masked_indices = [i for i, char in enumerate(masked_word.split()) if char == '_']\n",
    "predictions = predictor(embeddings[:, masked_indices, :])\n",
    "\n",
    "# Convert predictions to characters\n",
    "predicted_chars = [tokenizer.decode(torch.argmax(pred, dim=-1)) for pred in predictions]\n",
    "print(predicted_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CharacterPredictor(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size):\n",
    "        super(CharacterPredictor, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Example: Assume `vocab_size` is the size of your character vocabulary\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "embedding_dim = embeddings.shape[-1]\n",
    "predictor = CharacterPredictor(embedding_dim, vocab_size)\n",
    "\n",
    "# Predict masked characters\n",
    "masked_indices = [i for i, char in enumerate(masked_word.split()) if char == '_']\n",
    "predictions = predictor(embeddings[:, masked_indices, :])\n",
    "\n",
    "# Convert predictions to characters\n",
    "predicted_chars = [tokenizer.decode(torch.argmax(pred, dim=-1)) for pred in predictions]\n",
    "print(predicted_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed batch to CharacterBERT & get the embeddings\n",
    "embeddings_for_batch, _ = model(batch_ids)\n",
    "print(\"Output: \", embeddings_for_batch.size())\n",
    "embeddings_for_x = embeddings_for_batch[0]\n",
    "print('These are the embeddings produces by CharacterBERT (last transformer layer)')\n",
    "for token, embedding in zip(x, embeddings_for_x):\n",
    "    print(token, embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://stackoverflow.com/questions/69907682/what-are-differences-between-automodelforsequenceclassification-vs-automodel\n",
    "\n",
    "https://huggingface.co/learn/nlp-course/chapter3/4?fw=pt\n",
    "\n",
    "https://aclanthology.org/2020.coling-main.609.pdf\n",
    "\n",
    "https://github.com/helboukkouri/character-bert?tab=readme-ov-file#how-do-i-pre-train-characterbert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "character-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
