{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs\n",
    "# _ _ _ _ _\n",
    "\n",
    "# 250000 words\n",
    "\n",
    "# El potter no [MASK] novia\n",
    "\n",
    "# p _ t t e r\n",
    "\n",
    "\n",
    "# 1. Filtrar las palabras\n",
    "# 2. Palabras mas comunes\n",
    "# 3. Data colador\n",
    "# 4. Penalize more small words??\n",
    "# 5. Podria concatenar las palabras adivinadas hasta ahora al final del classifier, es practicamente concatenar un one-hot vector \n",
    "# de 0-26 con 1 donde las palabras ya fueron adivinadas.\n",
    "# 6. Podria usar one-hot encodings 0-27 para los characteres\n",
    "# 7. El Test y el val tambien generalo de manera dinamica. No te preocupes por que distribucion tendra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict, Dataset\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Set a seed for all libraries\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word  word_length  unique_chars\n",
      "0      timpani            7             6\n",
      "1       worsle            6             6\n",
      "2        yinst            5             5\n",
      "3  grangerized           11             8\n",
      "4      matatua            7             4\n",
      "(204570, 3)\n",
      "(22730, 3)\n",
      "(130436, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets \n",
    "df_train = pd.read_csv('data/to_mask_train.csv', converters={'word': str})\n",
    "df_val = pd.read_csv('data/masked_val.csv', converters={'masked_word': str, 'labels': str, 'previous_guesses': eval})\n",
    "df_test = pd.read_csv('data/masked_test.csv', converters={'masked_word': str, 'labels': str, 'previous_guesses': eval})\n",
    "\n",
    "print(df_train.head())\n",
    "\n",
    "# Add spaces to the word in training\n",
    "df_train[\"word\"] = df_train[\"word\"].apply(lambda x: ' '.join(list(x)))\n",
    "\n",
    "# Add spaces to the masked_word and labels\n",
    "df_val[\"masked_word\"] = df_val[\"masked_word\"].apply(lambda x: ' '.join(list(x)))\n",
    "df_val[\"labels\"] = df_val[\"labels\"].apply(lambda x: ' '.join(list(x)))\n",
    "df_test[\"masked_word\"] = df_test[\"masked_word\"].apply(lambda x: ' '.join(list(x)))\n",
    "df_test[\"labels\"] = df_test[\"labels\"].apply(lambda x: ' '.join(list(x)))\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>word_length</th>\n",
       "      <th>unique_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t i m p a n i</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>w o r s l e</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>y i n s t</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g r a n g e r i z e d</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m a t a t u a</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    word  word_length  unique_chars\n",
       "0          t i m p a n i            7             6\n",
       "1            w o r s l e            6             6\n",
       "2              y i n s t            5             5\n",
       "3  g r a n g e r i z e d           11             8\n",
       "4          m a t a t u a            7             4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_word</th>\n",
       "      <th>labels</th>\n",
       "      <th>previous_guesses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h e _ _ c l _ t _ s</td>\n",
       "      <td>h e r a c l i t u s</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_ e _ o d _ _ _</td>\n",
       "      <td>m e r o d a c h</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_ _ g _ _ _</td>\n",
       "      <td>i n g i r t</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_ _ _ z u o k a</td>\n",
       "      <td>s h i z u o k a</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m _ _ t _ _ _ _ n n e _ _ e d</td>\n",
       "      <td>m u l t i c h a n n e l l e d</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     masked_word                         labels  \\\n",
       "0            h e _ _ c l _ t _ s            h e r a c l i t u s   \n",
       "1                _ e _ o d _ _ _                m e r o d a c h   \n",
       "2                    _ _ g _ _ _                    i n g i r t   \n",
       "3                _ _ _ z u o k a                s h i z u o k a   \n",
       "4  m _ _ t _ _ _ _ n n e _ _ e d  m u l t i c h a n n e l l e d   \n",
       "\n",
       "                                    previous_guesses  \n",
       "0  [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...  \n",
       "4  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['word'],\n",
       "        num_rows: 204570\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['masked_word', 'labels', 'previous_guesses'],\n",
       "        num_rows: 22730\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['masked_word', 'labels', 'previous_guesses'],\n",
       "        num_rows: 130436\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather all the data in a DatasetDict\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train[[\"word\"]]),\n",
    "    \"valid\": Dataset.from_pandas(df_val),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling, BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataCollatorForMLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer, mlm_probability=0.75, max_length=42):\n",
    "        super().__init__(tokenizer=tokenizer, mlm_probability=mlm_probability)\n",
    "        self.max_length = max_length\n",
    "        self.char_to_index = {chr(i + ord('a')): i for i in range(26)}\n",
    "\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "\n",
    "        # Convert characters to their corresponding numerical labels\n",
    "        labels = []\n",
    "        words = []\n",
    "        for example in examples:\n",
    "            example = example['word']\n",
    "            row_label = [self.char_to_index[char] for char in example.split()]\n",
    "            labels.append(row_label)\n",
    "            words.append(example)\n",
    "\n",
    "        # Pad labels to the same length (considering special tokens at the beginning and end of each label)\n",
    "        labels = [ [-100] + row_label + [-100] * (self.max_length - len(row_label) - 1) for row_label in labels]\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # Create a one-hot vector for the labels without consider the -100 values\n",
    "        prev_guess = torch.zeros((labels.shape[0], 26), dtype=torch.int64)\n",
    "        for i, label in enumerate(labels):\n",
    "            prev_guess[i][label[label != -100]] = 1\n",
    "\n",
    "        # TODO: I could add until 5 random previous guesses to the prev_guess tensor\n",
    "        #       to account for maximum number of mistakes made when playing Hangman\n",
    "\n",
    "        # Tokenize and pad the input examples\n",
    "        batch = self.tokenizer(words, truncation=True, padding='max_length', return_tensors=\"pt\", max_length=self.max_length)\n",
    "        \n",
    "        # Get the input_ids and apply masking\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        # labels = input_ids.clone()\n",
    "\n",
    "        for i, input_id in enumerate(input_ids):\n",
    "\n",
    "            # Get unique tokens\n",
    "            unique_tokens = torch.unique(input_id)\n",
    "            \n",
    "            # Filter Special Tokens by setting probability to 0.0\n",
    "            special_tokens_mask = self.tokenizer.get_special_tokens_mask(unique_tokens, already_has_special_tokens=True)\n",
    "            probabilities = torch.full(unique_tokens.shape, self.mlm_probability)\n",
    "            probabilities.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "            \n",
    "            # Filter the tokens to mask\n",
    "            tokens_to_mask = unique_tokens[torch.bernoulli(probabilities).bool()]\n",
    "\n",
    "            # Mask all instances of the chosen tokens\n",
    "            masked_indices = torch.zeros(input_id.shape, dtype=torch.bool)\n",
    "            for token in tokens_to_mask:\n",
    "                masked_indices[(input_id == token)] = True\n",
    "\n",
    "            labels[i][~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "            # 80% of the time, replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "            indices_replaced = torch.bernoulli(torch.full(input_id.shape, 0.8)).bool() & masked_indices\n",
    "            input_ids[i][indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "            # 10% of the time, replace masked input tokens with random word\n",
    "            # indices_random = torch.bernoulli(torch.full(input_id.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "            # random_words = torch.randint(len(self.tokenizer), input_id.shape, dtype=torch.long)\n",
    "            # input_ids[i][indices_random] = random_words[indices_random]\n",
    "\n",
    "            # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "\n",
    "        # Set to zero the values masked in the labels\n",
    "        for i, label in enumerate(labels):\n",
    "            prev_guess[i][label[label != -100]] = 0\n",
    "\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"prev_guess\"] = prev_guess\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word\n",
      "0    h e l l o\n",
      "1    w o r l d\n",
      "2    p a r i s\n",
      "3  b e r l i n\n",
      "Dataset({\n",
      "    features: ['word'],\n",
      "    num_rows: 4\n",
      "})\n",
      "tensor([[ 101, 1044, 1041, 1048, 1048, 1051,  102,    0,    0,    0],\n",
      "        [ 101,  103,  103, 1054, 1048, 1040,  102,    0,    0,    0]])\n",
      "tensor([[-100,    7, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,   22,   14,   17, -100,    3, -100, -100, -100, -100]])\n",
      "tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# TOY EXAMPLE\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the custom data collator\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer, mlm_probability=0.5, max_length=10)\n",
    "\n",
    "# Toy dataset\n",
    "toy_data = {\n",
    "    \"word\": [\"h e l l o\", \"w o r l d\", \"p a r i s\", \"b e r l i n\"]\n",
    "}\n",
    "df_temp = pd.DataFrame(toy_data)\n",
    "print(df_temp)\n",
    "\n",
    "# Tokenize the toy dataset\n",
    "TensorDataset = Dataset.from_pandas(df_temp)\n",
    "print(TensorDataset)\n",
    "toy_dataloader = DataLoader(TensorDataset, batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "# Print the first batch\n",
    "for batch in toy_dataloader:\n",
    "    print(batch['input_ids'])\n",
    "    print(batch['labels'])\n",
    "    print(batch['prev_guess'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.int64\n",
      "labels torch.int64\n",
      "prev_guess torch.int64\n",
      "input_ids tensor([[ 101, 1056, 1045, 1049, 1052, 1037,  103, 1045,  102,    0],\n",
      "        [ 101,  103,  103, 1054,  103,  103, 1041,  102,    0,    0],\n",
      "        [ 101, 1061,  103, 1050, 1055, 1056,  102,    0,    0,    0],\n",
      "        [ 101,  103,  103,  103, 1050,  103, 1041,  103, 1045,  103],\n",
      "        [ 101,  103, 1037, 1056, 1037, 1056,  103, 1037,  102,    0],\n",
      "        [ 101,  103, 1045,  103,  103,  103, 1054, 1045,  103,  103],\n",
      "        [ 101,  103, 1041, 1039, 1051, 1048, 1048, 1041,  103, 1056],\n",
      "        [ 101, 1039,  103, 1055,  103,  103,  103,  103, 1040,  103],\n",
      "        [ 101,  103,  103,  103, 1050, 1047,  102,    0,    0,    0],\n",
      "        [ 101, 1055,  103, 1054,  103,  103, 1051,  103, 1037, 1054]]) torch.Size([64, 42])\n",
      "labels tensor([[-100, -100, -100, -100, -100, -100,   13, -100, -100, -100],\n",
      "        [-100,   22,   14, -100,   18,   11, -100, -100, -100, -100],\n",
      "        [-100, -100,    8, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    6,   17,    0, -100,    6, -100,   17, -100,   25],\n",
      "        [-100,   12, -100, -100, -100, -100,   20, -100, -100, -100],\n",
      "        [-100,    7,    8,   18,   19,   14, -100,    8,    4,   19],\n",
      "        [-100,   17, -100,    2, -100, -100, -100, -100,    2, -100],\n",
      "        [-100, -100,   14, -100,   24,   12,   12,    4, -100,    8],\n",
      "        [-100,   18,   19,    8, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,   18,    4, -100,    8,    2, -100,    2,    0, -100]]) torch.Size([64, 42])\n",
      "prev_guess tensor([[1, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
      "        [1, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) torch.Size([64, 26])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the custom data collator\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer, mlm_probability=0.5, max_length=42)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    data['train'], \n",
    "    batch_size=64,\n",
    "    collate_fn = data_collator\n",
    ")\n",
    "\n",
    "# Prepare a batch using the custom data collator\n",
    "for batch in train_dataloader:\n",
    "    print(\"input_ids\", batch['input_ids'].dtype)\n",
    "    print(\"labels\", batch['labels'].dtype)\n",
    "    print(\"prev_guess\", batch['prev_guess'].dtype)\n",
    "\n",
    "    print(\"input_ids\", batch['input_ids'][:10,:10], batch['input_ids'].shape)\n",
    "    print(\"labels\", batch['labels'][:10,:10], batch['labels'].shape)\n",
    "    print(\"prev_guess\", batch['prev_guess'][:10,:10], batch['prev_guess'].shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9/9 [00:00<00:00, 282.69 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'attention_mask', 'prev_guess'],\n",
      "    num_rows: 9\n",
      "})\n",
      "tensor([ 101, 1044, 1041,  103,  103, 1039, 1048,  103, 1056,  103, 1055,  102,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "tensor([-100, -100, -100,   17,    0, -100, -100,    8, -100,   20, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "torch.Size([9, 42])\n",
      "torch.Size([9, 42])\n",
      "torch.Size([9, 26])\n",
      "input_ids torch.int64\n",
      "labels torch.int64\n",
      "prev_guess torch.int64\n",
      "input_ids tensor([ 101, 1044, 1041,  103,  103, 1039, 1048,  103, 1056,  103, 1055,  102,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0]) torch.Size([3, 42])\n",
      "labels tensor([-100, -100, -100,   17,    0, -100, -100,    8, -100,   20, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100]) torch.Size([3, 42])\n",
      "prev_guess tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0]) torch.Size([3, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class FixedTokenizer:\n",
    "    def __init__(self, tokenizer, max_length=42):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.char_to_index = {chr(i + ord('a')): i for i in range(26)}\n",
    "\n",
    "    def __call__(self, examples):\n",
    "\n",
    "        masked_word = examples['masked_word']\n",
    "        labels_word = examples['labels']\n",
    "        prev_guesses = examples['previous_guesses']\n",
    "\n",
    "        # Create a labels tensor with all the values to -100\n",
    "        labels_batch = torch.full((len(masked_word),self.max_length,), -100, dtype=torch.int64)\n",
    "\n",
    "        for i in range(len(masked_word)):\n",
    "            # Replace the underscore in masked_word with the special [MASK] token\n",
    "            masked_word[i] = masked_word[i].replace('_', '[MASK]')\n",
    "\n",
    "        # Tokenize the masked_words\n",
    "        batch = self.tokenizer(masked_word, truncation=True, padding='max_length', return_tensors=\"pt\", max_length=self.max_length)\n",
    "\n",
    "        # Create the labels per word\n",
    "        for i in range(len(masked_word)):\n",
    "\n",
    "            # Split labels_word[i] into a list of characters considering they are separated by spaces\n",
    "            labels_word[i] = labels_word[i].split()\n",
    "            masked_word[i] = masked_word[i].split()\n",
    "\n",
    "\n",
    "            # Convert labels_word to their corresponding numerical labels\n",
    "            for j, char in enumerate(labels_word[i]):\n",
    "                if masked_word[i][j] == '[MASK]':\n",
    "                    labels_batch[i][j + 1] = self.char_to_index[char]\n",
    "\n",
    "        prev_guesses_batch = torch.tensor(prev_guesses, dtype=torch.int64)\n",
    "\n",
    "        batch['prev_guess'] = prev_guesses_batch\n",
    "        batch['labels'] = labels_batch\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example usage with first 9 word\n",
    "df = df_val[:9]\n",
    "\n",
    "data_set = Dataset.from_pandas(df)\n",
    "\n",
    "# Initialize the data collator\n",
    "tokenizer = FixedTokenizer(tokenizer, max_length=42)\n",
    "\n",
    "# Process the examples\n",
    "data_set = data_set.map(tokenizer, batched=True, batch_size=3)\n",
    "data_set.set_format(\"torch\")\n",
    "data_set = data_set.remove_columns(['masked_word','previous_guesses', 'token_type_ids'])\n",
    "\n",
    "print(data_set)\n",
    "\n",
    "print(data_set['input_ids'][0])\n",
    "print(data_set['labels'][0])\n",
    "print(data_set['prev_guess'][0])\n",
    "\n",
    "# Print the shapes of the processed data\n",
    "print(data_set['input_ids'].shape)\n",
    "print(data_set['labels'].shape)\n",
    "print(data_set['prev_guess'].shape)\n",
    "\n",
    "# Data loader\n",
    "val_dataloader = DataLoader(\n",
    "    data_set, \n",
    "    batch_size=3,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Check the first batch of the validation data loader\n",
    "for batch in val_dataloader:\n",
    "    print(\"input_ids\", batch['input_ids'].dtype)\n",
    "    print(\"labels\", batch['labels'].dtype)\n",
    "    print(\"prev_guess\", batch['prev_guess'].dtype)\n",
    "\n",
    "    print(\"input_ids\", batch['input_ids'][0], batch['input_ids'].shape)\n",
    "    print(\"labels\", batch['labels'][0], batch['labels'].shape)\n",
    "    print(\"prev_guess\", batch['prev_guess'][0], batch['prev_guess'].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 22730/22730 [00:08<00:00, 2625.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Validation and Test datasets\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Initialize the data collator\n",
    "tokenizer = FixedTokenizer(tokenizer, max_length=42)\n",
    "\n",
    "\n",
    "# Tokenize the val dataset\n",
    "tokenized_val_data = data['valid'].map(tokenizer, batched=True, batch_size=3)\n",
    "tokenized_val_data.set_format(\"torch\")\n",
    "tokenized_val_data = tokenized_val_data.remove_columns(['masked_word','previous_guesses', 'token_type_ids'])\n",
    "\n",
    "# Tokenize the test dataset\n",
    "# tokenized_test_data = data['test'].map(tokenizer, batched=True, batch_size=3)\n",
    "# tokenized_test_data.set_format(\"torch\")\n",
    "# tokenized_test_data = tokenized_test_data.remove_columns(['masked_word','previous_guesses', 'token_type_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desing the model for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset,DatasetDict\n",
    "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanNet(nn.Module):\n",
    "  def __init__(self,checkpoint, vocab_size = 26, hidden_ffn_size = 410, unfreeze_layers = 0): \n",
    "    super(HangmanNet,self).__init__() \n",
    "    self.num_labels = vocab_size \n",
    "\n",
    "    #Load Model with given checkpoint and extract its body\n",
    "    self.model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "    \n",
    "    # Freeze all layers in the BERT model\n",
    "    for param in self.model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the last `unfreeze_layers` layers\n",
    "    if unfreeze_layers > 0:\n",
    "        for layer in self.model.encoder.layer[-unfreeze_layers:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Linear(768 + vocab_size, hidden_ffn_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_ffn_size, 26)\n",
    "    )\n",
    "    \n",
    "    # self.classifier = nn.Linear(768 + vocab_size,vocab_size) # load and initialize weights\n",
    "  \n",
    "  def forward(self, input_ids=None, attention_mask=None, labels=None, prev_guess=None,\n",
    "              token_type_ids=None):\n",
    "      outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "      \n",
    "      sequence_output = outputs.last_hidden_state  # (batch_size, sequence_length, hidden_size)\n",
    "      sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "      # Concatenate the previous guesses to the sequence_output\n",
    "      # (batch_size, sequence_length, hidden_size + vocab_size)\n",
    "      sequence_output = torch.cat((sequence_output, prev_guess.unsqueeze(1).repeat(1, sequence_output.shape[1], 1)), dim=2)\n",
    "\n",
    "      logits = self.classifier(sequence_output)  # (batch_size, sequence_length, num_labels)\n",
    "\n",
    "      loss = None\n",
    "      if labels is not None:\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "          # NOTE: I already has the labels in active logits representation\n",
    "\n",
    "          # Mask the logits to zero out probabilities of previously guessed characters by considering the one-hot encoding in prev guesses\n",
    "          logits[prev_guess.unsqueeze(1).repeat(1, sequence_output.shape[1], 1) == 1] = -float(\"inf\")\n",
    "          active_logits = logits.view(-1, self.num_labels)\n",
    "\n",
    "        #   active_labels = torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))\n",
    "          loss = loss_fct(active_logits, labels.view(-1))\n",
    "        \n",
    "      return TokenClassifierOutput(logits=logits, loss=loss, hidden_states=outputs.hidden_states,attentions=outputs.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1326, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([64, 42, 26])\n"
     ]
    }
   ],
   "source": [
    "# Check the model output\n",
    "device = \"cpu\"\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "model = HangmanNet(checkpoint=checkpoint, vocab_size = 26, unfreeze_layers = 1).to(device)\n",
    "\n",
    "# Prepare a batch using the custom data collator\n",
    "for batch in train_dataloader:\n",
    "\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "    prev_guess = batch['prev_guess']\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, prev_guess=prev_guess)\n",
    "    print(outputs.loss)\n",
    "    print(outputs.logits.shape)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "model.encoder.layer.10.attention.self.query.weight\n",
      "model.encoder.layer.10.attention.self.query.bias\n",
      "model.encoder.layer.10.attention.self.key.weight\n",
      "model.encoder.layer.10.attention.self.key.bias\n",
      "model.encoder.layer.10.attention.self.value.weight\n",
      "model.encoder.layer.10.attention.self.value.bias\n",
      "model.encoder.layer.10.attention.output.dense.weight\n",
      "model.encoder.layer.10.attention.output.dense.bias\n",
      "model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.10.intermediate.dense.weight\n",
      "model.encoder.layer.10.intermediate.dense.bias\n",
      "model.encoder.layer.10.output.dense.weight\n",
      "model.encoder.layer.10.output.dense.bias\n",
      "model.encoder.layer.10.output.LayerNorm.weight\n",
      "model.encoder.layer.10.output.LayerNorm.bias\n",
      "model.encoder.layer.11.attention.self.query.weight\n",
      "model.encoder.layer.11.attention.self.query.bias\n",
      "model.encoder.layer.11.attention.self.key.weight\n",
      "model.encoder.layer.11.attention.self.key.bias\n",
      "model.encoder.layer.11.attention.self.value.weight\n",
      "model.encoder.layer.11.attention.self.value.bias\n",
      "model.encoder.layer.11.attention.output.dense.weight\n",
      "model.encoder.layer.11.attention.output.dense.bias\n",
      "model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.11.intermediate.dense.weight\n",
      "model.encoder.layer.11.intermediate.dense.bias\n",
      "model.encoder.layer.11.output.dense.weight\n",
      "model.encoder.layer.11.output.dense.bias\n",
      "model.encoder.layer.11.output.LayerNorm.weight\n",
      "model.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.2.weight\n",
      "classifier.2.bias\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HangmanNet(checkpoint=checkpoint, vocab_size = 26, unfreeze_layers = 2).to(device)\n",
    "\n",
    "# Print the trainable parameters of the model\n",
    "print(\"Trainable parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zosov/anaconda3/envs/character-bert/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 50\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     \"linear\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=num_training_steps,\n",
    "# )\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "data_collator = CustomDataCollatorForMLM(tokenizer, mlm_probability=0.5, max_length=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    data[\"train\"], \n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=True)\n",
    "\n",
    "# Probably add shuffle\n",
    "    \n",
    "eval_dataloader = DataLoader(tokenized_val_data,\n",
    "                             batch_size=64,\n",
    "                             num_workers=8,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. Anadir otra capa al clasifier + anadir al output la prior encoding info\n",
    "# 2. Eliminar el scheduler\n",
    "# 3. Descongelar otra capa\n",
    "# 3. añadir la otra prior information basada en frecuencias\n",
    "# 4. Set la probabilidad aleatoria en lugar de fija\n",
    "# 4. Añadir el early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50: 100%|██████████| 3197/3197 [05:14<00:00, 10.16it/s]\n",
      "Validation Epoch 1/50: 100%|██████████| 356/356 [00:26<00:00, 13.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Training Loss: 1.8215\n",
      "Validation Loss: 2.1229\n",
      "Validation Accuracy: 0.2981\n",
      "Validation F1 Score: 0.2034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/50: 100%|██████████| 3197/3197 [05:25<00:00,  9.83it/s]\n",
      "Validation Epoch 2/50: 100%|██████████| 356/356 [00:26<00:00, 13.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "Training Loss: 1.6575\n",
      "Validation Loss: 2.0509\n",
      "Validation Accuracy: 0.3179\n",
      "Validation F1 Score: 0.2296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/50: 100%|██████████| 3197/3197 [05:24<00:00,  9.85it/s]\n",
      "Validation Epoch 3/50: 100%|██████████| 356/356 [00:26<00:00, 13.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "Training Loss: 1.6090\n",
      "Validation Loss: 2.0053\n",
      "Validation Accuracy: 0.3310\n",
      "Validation F1 Score: 0.2485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/50: 100%|██████████| 3197/3197 [05:24<00:00,  9.87it/s]\n",
      "Validation Epoch 4/50: 100%|██████████| 356/356 [00:26<00:00, 13.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "Training Loss: 1.5726\n",
      "Validation Loss: 1.9744\n",
      "Validation Accuracy: 0.3399\n",
      "Validation F1 Score: 0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/50: 100%|██████████| 3197/3197 [05:24<00:00,  9.84it/s]\n",
      "Validation Epoch 5/50: 100%|██████████| 356/356 [00:26<00:00, 13.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "Training Loss: 1.5434\n",
      "Validation Loss: 1.9570\n",
      "Validation Accuracy: 0.3459\n",
      "Validation F1 Score: 0.2689\n",
      "Model saved to model_and_logs/model_epoch_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/50: 100%|██████████| 3197/3197 [05:23<00:00,  9.87it/s]\n",
      "Validation Epoch 6/50: 100%|██████████| 356/356 [00:26<00:00, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "Training Loss: 1.5229\n",
      "Validation Loss: 1.9299\n",
      "Validation Accuracy: 0.3551\n",
      "Validation F1 Score: 0.2805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/50: 100%|██████████| 3197/3197 [05:25<00:00,  9.84it/s]\n",
      "Validation Epoch 7/50: 100%|██████████| 356/356 [00:26<00:00, 13.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "Training Loss: 1.5063\n",
      "Validation Loss: 1.9149\n",
      "Validation Accuracy: 0.3604\n",
      "Validation F1 Score: 0.2846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/50: 100%|██████████| 3197/3197 [05:24<00:00,  9.85it/s]\n",
      "Validation Epoch 8/50: 100%|██████████| 356/356 [00:26<00:00, 13.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "Training Loss: 1.4926\n",
      "Validation Loss: 1.9018\n",
      "Validation Accuracy: 0.3635\n",
      "Validation F1 Score: 0.2866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/50: 100%|██████████| 3197/3197 [05:24<00:00,  9.84it/s]\n",
      "Validation Epoch 9/50: 100%|██████████| 356/356 [00:26<00:00, 13.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "Training Loss: 1.4789\n",
      "Validation Loss: 1.8885\n",
      "Validation Accuracy: 0.3685\n",
      "Validation F1 Score: 0.2967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/50: 100%|██████████| 3197/3197 [05:25<00:00,  9.83it/s]\n",
      "Validation Epoch 10/50: 100%|██████████| 356/356 [00:26<00:00, 13.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "Training Loss: 1.4676\n",
      "Validation Loss: 1.8763\n",
      "Validation Accuracy: 0.3726\n",
      "Validation F1 Score: 0.2993\n",
      "Model saved to model_and_logs/model_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/50: 100%|██████████| 3197/3197 [05:24<00:00,  9.86it/s]\n",
      "Validation Epoch 11/50: 100%|██████████| 356/356 [00:26<00:00, 13.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "Training Loss: 1.4570\n",
      "Validation Loss: 1.8656\n",
      "Validation Accuracy: 0.3748\n",
      "Validation F1 Score: 0.3030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/50: 100%|██████████| 3197/3197 [05:23<00:00,  9.87it/s]\n",
      "Validation Epoch 12/50: 100%|██████████| 356/356 [00:26<00:00, 13.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50\n",
      "Training Loss: 1.4480\n",
      "Validation Loss: 1.8583\n",
      "Validation Accuracy: 0.3791\n",
      "Validation F1 Score: 0.3094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/50: 100%|██████████| 3197/3197 [05:25<00:00,  9.84it/s]\n",
      "Validation Epoch 13/50: 100%|██████████| 356/356 [00:26<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "Training Loss: 1.4372\n",
      "Validation Loss: 1.8490\n",
      "Validation Accuracy: 0.3818\n",
      "Validation F1 Score: 0.3119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/50: 100%|██████████| 3197/3197 [05:24<00:00,  9.85it/s]\n",
      "Validation Epoch 14/50: 100%|██████████| 356/356 [00:26<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      "Training Loss: 1.4311\n",
      "Validation Loss: 1.8405\n",
      "Validation Accuracy: 0.3816\n",
      "Validation F1 Score: 0.3143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/50: 100%|██████████| 3197/3197 [05:24<00:00,  9.84it/s]\n",
      "Validation Epoch 15/50: 100%|██████████| 356/356 [00:26<00:00, 13.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "Training Loss: 1.4259\n",
      "Validation Loss: 1.8359\n",
      "Validation Accuracy: 0.3853\n",
      "Validation F1 Score: 0.3163\n",
      "Model saved to model_and_logs/model_epoch_15.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/50:  87%|████████▋ | 2778/3197 [04:42<00:42,  9.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# lr_scheduler.step()\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 35\u001b[0m     total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m     38\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir='model_and_logs', filename_suffix=\"hangman\")\n",
    "\n",
    "# Initialize metrics storage\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_f1_scores = []\n",
    "\n",
    "# Define the save frequency\n",
    "save_frequency = 5  # Save model every n epochs, adjust this as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        # lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        flat_predictions = predictions.view(-1)\n",
    "        flat_references = batch[\"labels\"].view(-1)\n",
    "        flat_attention_mask = batch[\"labels\"].view(-1) != -100\n",
    "\n",
    "        active_predictions = flat_predictions[flat_attention_mask]\n",
    "        active_references = flat_references[flat_attention_mask]\n",
    "\n",
    "        all_predictions.extend(active_predictions.cpu().numpy())\n",
    "        all_references.extend(active_references.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(eval_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    val_accuracy = accuracy_score(all_references, all_predictions)\n",
    "    val_f1 = f1_score(all_references, all_predictions, average=\"macro\")\n",
    "\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "    # Save the model every n epochs\n",
    "    if (epoch + 1) % save_frequency == 0:\n",
    "        save_dir = \"model_and_logs\"\n",
    "        model_save_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pth\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save the final model\n",
    "save_dir = \"model_and_logs\"\n",
    "model_save_path = os.path.join(save_dir, \"model_final.pth\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Final model saved to {model_save_path}\")\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning and CharacterPredictor Head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://stackoverflow.com/questions/69907682/what-are-differences-between-automodelforsequenceclassification-vs-automodel\n",
    "\n",
    "https://huggingface.co/learn/nlp-course/chapter3/4?fw=pt\n",
    "\n",
    "https://aclanthology.org/2020.coling-main.609.pdf\n",
    "\n",
    "https://github.com/helboukkouri/character-bert?tab=readme-ov-file#how-do-i-pre-train-characterbert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "character-bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
